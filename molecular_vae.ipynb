{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Molecular VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [17:09:43] Enabling RDKit 2019.09.1 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "import torch.utils.data as tud\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# chemistry\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "IPythonConsole.ipython_useSVG=True  #set this to False if you want PNGs instead of SVGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ChEMBL ID</th>\n",
       "      <th>Smiles</th>\n",
       "      <th>AlogP</th>\n",
       "      <th>QED Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL2333117</td>\n",
       "      <td>CC(C)Nc1c(C(N)=O)nnc2ccc(-c3cnn(C)c3)cc12</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL1189585</td>\n",
       "      <td>CC1C(=O)NC2=Nc3sc4c(c3CN21)CCCC4</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL4089494</td>\n",
       "      <td>CNC(=O)c1ccc(NC(=O)Nc2ccc(-c3nc(N4CCOCC4)c4ncc...</td>\n",
       "      <td>3.53</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL1189590</td>\n",
       "      <td>CN(C)c1nccc2c1nnn2Cc1ccccc1F</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3927722</td>\n",
       "      <td>Cc1noc(C)c1Cn1cc(NC(=O)Cc2ccco2)cn1</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ChEMBL ID                                             Smiles  AlogP  \\\n",
       "0  CHEMBL2333117          CC(C)Nc1c(C(N)=O)nnc2ccc(-c3cnn(C)c3)cc12   1.95   \n",
       "1  CHEMBL1189585                   CC1C(=O)NC2=Nc3sc4c(c3CN21)CCCC4   1.95   \n",
       "2  CHEMBL4089494  CNC(=O)c1ccc(NC(=O)Nc2ccc(-c3nc(N4CCOCC4)c4ncc...   3.53   \n",
       "3  CHEMBL1189590                       CN(C)c1nccc2c1nnn2Cc1ccccc1F   2.08   \n",
       "4  CHEMBL3927722                Cc1noc(C)c1Cn1cc(NC(=O)Cc2ccco2)cn1   2.31   \n",
       "\n",
       "   QED Weighted  \n",
       "0          0.77  \n",
       "1          0.78  \n",
       "2          0.40  \n",
       "3          0.73  \n",
       "4          0.78  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chembl = pd.read_csv(\"data/cleaned_dataset.csv\")\n",
    "chembl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation & encoding\n",
    "\n",
    "Steps to prepare VAE input data:\n",
    "1. SMILES tokenization (add start/end tokens)\n",
    "2. SMILES token encoding (convert to integer indecies) -> build a vocabulary\n",
    "3. Set up a SMILES dataset class (for feeding batches of data to the VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility classes/functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0,
     133,
     170
    ]
   },
   "outputs": [],
   "source": [
    "class SMILESTokenizer(object):\n",
    "    \n",
    "    def __init__(self, pattern=\"(Br|Cl)\", start=\"<sos>\", end=\"<eos>\"):\n",
    "        self.pattern = re.compile(pattern)\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        \n",
    "    def getStart(self):\n",
    "        return self.start\n",
    "    \n",
    "    def getEnd(self):\n",
    "        return self.end\n",
    "    \n",
    "    def getPattern(self):\n",
    "        \"\"\"Returns compiled regex pattern for multi-character tokens\"\"\"\n",
    "        return self.pattern\n",
    "\n",
    "    def tokenize(self, smi:str, use_start_end:bool=True) -> list:\n",
    "        \"\"\"Tokenizes an input SMILES string\"\"\"\n",
    "        start = [self.start] if self.start else []\n",
    "        end = [self.end] if self.end else []\n",
    "        if not self.pattern:\n",
    "            if use_start_end:\n",
    "                return start + list(smi) + end\n",
    "            else:\n",
    "                return list(smi)\n",
    "        # split input SMILES string using the supplied regex pattern    \n",
    "        splitted = self.pattern.split(smi)\n",
    "        tokens = []\n",
    "        for i, s in enumerate(splitted):\n",
    "            # make sure Br and Cl are treated as a single token\n",
    "            if i % 2 == 0:\n",
    "                tokens.extend(list(s))\n",
    "            else:\n",
    "                tokens.append(s)\n",
    "        if use_start_end:\n",
    "            return start + tokens + end\n",
    "        else:\n",
    "            return tokens\n",
    "\n",
    "    def untokenize(self, tokens:list) -> str:\n",
    "        \"\"\"Concatenates a list of tokens into a SMILES string\"\"\"\n",
    "        smiles = \"\"\n",
    "        for t in tokens:\n",
    "            if self.start and t == self.start:\n",
    "                continue\n",
    "            if self.end and t == self.end:\n",
    "                continue\n",
    "            else:\n",
    "                smiles += t\n",
    "        return smiles\n",
    "    \n",
    "\n",
    "class SMILESVocabulary(object):\n",
    "    \"\"\"Keeps track of string tokens and their associated integer indecies\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._tokens_idxs = {} # vocabulary\n",
    "        self._current_idx = 0 # currently available index\n",
    "\n",
    "    def __getitem__(self, token_or_idx):\n",
    "        return self._tokens_idxs[token_or_idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._tokens_idxs) // 2\n",
    "    \n",
    "    def add(self, token:str):\n",
    "        assert type(token) == str, \"Token must be of type string.\"\n",
    "        if token not in self._tokens_idxs:\n",
    "            self._tokens_idxs[token] = self._current_idx\n",
    "            self._tokens_idxs[self._current_idx] = token\n",
    "            # update first available index\n",
    "            self._current_idx += 1\n",
    "            \n",
    "    def tokens(self):\n",
    "        \"\"\"Returns a list of all tokens in the vocabulary\"\"\"\n",
    "        return [t for t in self._tokens_idxs if type(t) == str]\n",
    "\n",
    "    def update(self, tokens:list):\n",
    "        \"\"\"Updates the vocabulary with an iterable of tokens\"\"\"\n",
    "        for t in tokens:\n",
    "            self.add(t)\n",
    "        \n",
    "    def encode(self, tokens:list) -> list:\n",
    "        \"\"\"Encodes a list of tokens as a list of integer indecies\"\"\"\n",
    "        return [self._tokens_idxs[t] for t in tokens]\n",
    "    \n",
    "    def decode(self, indecies:list) -> list:\n",
    "        \"\"\"Decodes a list of interger indecies as a list of tokens\"\"\"\n",
    "        return [self._tokens_idxs[t] for t in indecies]\n",
    "\n",
    "    def build(self, smiles:list, tokenizer:SMILESTokenizer) -> None:\n",
    "        \"\"\"\n",
    "        Builds a vocabulary using a list of SMILES and\n",
    "        an instance of a Tokenizer object. Any existing\n",
    "        vocabulary is reset.\n",
    "        -------------------------------\n",
    "        smiles - iterable of SMILES strings\n",
    "        tokenizer - instantiated SMILESTokenizer object\n",
    "        \n",
    "        returns None\n",
    "        \"\"\"\n",
    "        # reset current vocabulary\n",
    "        self.__init__()\n",
    "        # build new vocabulary\n",
    "        tokens = set()\n",
    "        for smi in smiles:\n",
    "            tokens.update(tokenizer.tokenize(smi, use_start_end=False))\n",
    "        # end-of-seq token gets idx 0 and start-of-seq token gets idx 1\n",
    "        self.update([tokenizer.getEnd(), tokenizer.getStart()] + sorted(tokens))\n",
    "        \n",
    "    def save(self, path):\n",
    "        \"\"\"Saves the vocabulary to disk\"\"\"\n",
    "        voc = [[k, v] for k, v in self._tokens_idxs.items() if isinstance(k, str)]\n",
    "        voc_df = pd.DataFrame(voc, columns=[\"token\", \"index\"])\n",
    "        voc_df.to_csv(path, index=False)\n",
    "    \n",
    "    def load(self, path):\n",
    "        \"\"\"Loads a stored vocabulary from disk\"\"\"\n",
    "        # reset current vocabulary\n",
    "        self.__init__()\n",
    "        # build vocabulary from csv file\n",
    "        voc_df = pd.read_csv(path)\n",
    "        for _, row in voc_df.iterrows():\n",
    "            token = row[\"token\"]\n",
    "            idx = row[\"index\"]\n",
    "            self._tokens_idxs[token] = idx\n",
    "            self._tokens_idxs[idx] = token\n",
    "            self._current_idx = max(self._current_idx, idx)\n",
    "        # update the currently available index\n",
    "        self._current_idx += 1\n",
    "\n",
    "\n",
    "class SMILESDataset(tud.Dataset):\n",
    "    \"\"\"Custom dataset class for producing batches of SMILES\"\"\"\n",
    "    \n",
    "    def __init__(self, smiles, vocabulary:SMILESVocabulary, tokenizer:SMILESTokenizer):\n",
    "        \"\"\"\n",
    "        Creates a dataset from an iterable of SMILES, a built vocabulary of tokens\n",
    "        and a SMILES tokenizer.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self._smiles = list(smiles)\n",
    "        self._vocabulary = vocabulary\n",
    "        self._tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns a molecule at index idx as an encoded SMILES tensor\"\"\"\n",
    "        smi = self._smiles[idx]\n",
    "        smi_enc = vocabulary.encode(tokenizer.tokenize(smi))\n",
    "        return torch.LongTensor(smi_enc)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._smiles)\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(enc_tensors):\n",
    "        \"\"\"\n",
    "        Pads encoded SMILES tensors to the same max. length using 0:s\n",
    "        The output tensor has shape: (batch_sz, max. length)\n",
    "        \"\"\"\n",
    "        batch_sz = len(enc_tensors)\n",
    "        max_len = max([t.size(0) for t in enc_tensors])\n",
    "        padded = torch.zeros((batch_sz, max_len), dtype=torch.long)\n",
    "        # pad encoded batch of SMILES\n",
    "        for i, t in enumerate(enc_tensors):\n",
    "            padded[i, :t.size(0)] = t\n",
    "        return padded\n",
    "\n",
    "\n",
    "def countTokens(smiles:list, tokenizer:SMILESTokenizer, tokenCol:str=\"token\", cntCol:str=\"cnt\") -> pd.DataFrame:\n",
    "    \"\"\"Computes the token frequency in the smiles iterable\"\"\"\n",
    "    token_cnts = {}\n",
    "    for smi in smiles:\n",
    "        # tokenize SMILES string\n",
    "        tokenized = tokenizer.tokenize(smi, use_start_end=False)\n",
    "        # count tokens\n",
    "        for t in tokenized:\n",
    "            try:\n",
    "                token_cnts[t] += 1\n",
    "            except KeyError:\n",
    "                token_cnts[t] = 1\n",
    "    return pd.DataFrame([[t, c] for t, c in token_cnts.items()], columns=[tokenCol, cntCol])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze token frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C</td>\n",
       "      <td>12896279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(</td>\n",
       "      <td>7025189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>)</td>\n",
       "      <td>7025189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N</td>\n",
       "      <td>2682981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c</td>\n",
       "      <td>18431345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>4604265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>=</td>\n",
       "      <td>2904352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>O</td>\n",
       "      <td>4388805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>n</td>\n",
       "      <td>2124346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>3590237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-</td>\n",
       "      <td>576826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>1781615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>s</td>\n",
       "      <td>190809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>530543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>F</td>\n",
       "      <td>577815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>o</td>\n",
       "      <td>190476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Cl</td>\n",
       "      <td>340814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[</td>\n",
       "      <td>1253085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@</td>\n",
       "      <td>1286887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>H</td>\n",
       "      <td>936158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>]</td>\n",
       "      <td>1253085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>S</td>\n",
       "      <td>418728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5</td>\n",
       "      <td>98750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>#</td>\n",
       "      <td>109078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Br</td>\n",
       "      <td>70038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>/</td>\n",
       "      <td>345947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>\\</td>\n",
       "      <td>78135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6</td>\n",
       "      <td>11356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7</td>\n",
       "      <td>1131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>P</td>\n",
       "      <td>20234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>p</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>+</td>\n",
       "      <td>102796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>I</td>\n",
       "      <td>9080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>%</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   token       cnt\n",
       "0      C  12896279\n",
       "1      (   7025189\n",
       "2      )   7025189\n",
       "3      N   2682981\n",
       "4      c  18431345\n",
       "5      1   4604265\n",
       "6      =   2904352\n",
       "7      O   4388805\n",
       "8      n   2124346\n",
       "9      2   3590237\n",
       "10     -    576826\n",
       "11     3   1781615\n",
       "12     s    190809\n",
       "13     4    530543\n",
       "14     F    577815\n",
       "15     o    190476\n",
       "16    Cl    340814\n",
       "17     [   1253085\n",
       "18     @   1286887\n",
       "19     H    936158\n",
       "20     ]   1253085\n",
       "21     S    418728\n",
       "22     5     98750\n",
       "23     #    109078\n",
       "24    Br     70038\n",
       "25     /    345947\n",
       "26     \\     78135\n",
       "27     6     11356\n",
       "28     8       516\n",
       "29     7      1131\n",
       "30     P     20234\n",
       "31     p        17\n",
       "32     +    102796\n",
       "33     I      9080\n",
       "34     9        17\n",
       "35     %         8\n",
       "36     0         4"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_cnts = countTokens(chembl[\"Smiles\"], tokenizer)\n",
    "token_cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot token distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "_ = sns.barplot(data=token_cnts.sort_values(\"cnt\", ascending=False), x=\"token\", y=\"cnt\", color=\"steelblue\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SMILESTokenizer()\n",
    "vocabulary = SMILESVocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary.build(chembl[\"Smiles\"], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save vocabulary\n",
    "# vocabulary.save(\"data/vocabulary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vocabulary\n",
    "vocabulary.load(\"data/vocabulary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SMILESDataset(chembl[\"Smiles\"], vocabulary, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test producing batches with dataloader\n",
    "dataloader = tud.DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=SMILESDataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dloader_iter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 27, 31,  ...,  0,  0,  0],\n",
       "        [ 1, 22, 28,  ...,  0,  0,  0],\n",
       "        [ 1, 22, 28,  ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 1, 22,  2,  ...,  0,  0,  0],\n",
       "        [ 1, 28, 19,  ...,  0,  0,  0],\n",
       "        [ 1, 22, 34,  ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(dloader_iter)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 98])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN modules\n",
    "\n",
    "The variational autoencoder consists of the following components:\n",
    "1. SMILES encoder: maps input sequences to a latent vector z\n",
    "2. Decoder: decodes a latent vector z into a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MolecularVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes a sequence as a probability distribution over a latent space - z\n",
    "    and samples from this probability distribution.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_sz:int, embedding_dim:int, hidden_dim:int, latent_dim:int, sos_idx:int, eos_idx:int, \n",
    "                 rnn_layers:int=1, bidirectional:bool=True, pad_idx:int=0, dropout:float=0.0):\n",
    "        \"\"\"Parameter initialization\"\"\"\n",
    "        super().__init__()\n",
    "        # module params\n",
    "        self.vocab_sz = vocab_sz\n",
    "        self.sos_idx = sos_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.pad_idx = pad_idx\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.rnn_layers = rnn_layers\n",
    "        self.hidden_factor = (2 if self.bidirectional else 1) * self.rnn_layers\n",
    "        \n",
    "        # embedding layer (used by both encoder and decoder)\n",
    "        self.embedding = nn.Embedding(self.vocab_sz, self.embedding_dim, padding_idx=self.pad_idx)\n",
    "        \n",
    "        # encoder RNN\n",
    "        self.encoder_rnn = nn.GRU(self.embedding_dim, self.hidden_dim, num_layers=self.rnn_layers, \n",
    "                          batch_first=True, dropout=dropout, bidirectional=self.bidirectional)\n",
    "        \n",
    "        # linear layers for computing the params of the latent vector z distribution\n",
    "        # (diagonal multivariate gaussian) from the hidden state vector of the RNN\n",
    "        self.hidden2mean = nn.Linear(self.hidden_dim * self.hidden_factor, self.latent_dim)\n",
    "        self.hidden2logv = nn.Linear(self.hidden_dim * self.hidden_factor, self.latent_dim)\n",
    "        \n",
    "        # linear layers for computing the decoder hidden vector from the latent vector\n",
    "        self.latent2hidden = nn.Linear(self.latent_dim, self.hidden_dim * self.hidden_factor)\n",
    "        \n",
    "        # decoder layers\n",
    "        self.decoder_rnn = nn.GRU(self.embedding_dim, self.hidden_dim, num_layers=self.rnn_layers, \n",
    "                  batch_first=True, dropout=dropout, bidirectional=self.bidirectional)\n",
    "        self.outputs2vocab = nn.Linear(self.hidden_dim * (2 if self.bidirectional else 1), self.vocab_sz)\n",
    "        \n",
    "    def forward(self, input_seqs:torch.Tensor) -> (torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor):\n",
    "        \"\"\"\n",
    "        Performs the encoding and reparametrization step.\n",
    "        ------------------------\n",
    "        input_seqs: input batch of sequences (batch size, seq. length)\n",
    "        \n",
    "        returns (z, mean, logv)\n",
    "        \"\"\"\n",
    "        input_embeddings = self.embedding(input_seqs)\n",
    "        # run sequence through the encoder\n",
    "        mean, logv, stdev = self.encode(input_embeddings)\n",
    "        # sample z from the posterior distribution\n",
    "        z = self.samplePosterior(mean, stdev)\n",
    "        # run through the decoder\n",
    "        logits = self.decode(z, input_embeddings)\n",
    "        return logits, z, mean, logv\n",
    "    \n",
    "    def encode(self, input_embeddings:torch.Tensor) -> (torch.Tensor, torch.Tensor, torch.Tensor):\n",
    "        \"\"\"Encodes a sequence as parametrized posterior distribution over the latent space - z\"\"\"\n",
    "        _, hidden = self.encoder_rnn(input_embeddings)\n",
    "        # flatten RNN output\n",
    "        hidden = hidden.view(-1, self.hidden_factor * self.hidden_dim)\n",
    "        # reparametrize (compute posterior distribution params)\n",
    "        mean = self.hidden2mean(hidden)\n",
    "        logv = self.hidden2logv(hidden)\n",
    "        stdev = torch.exp(logv / 2)\n",
    "        return mean, logv, stdev\n",
    "        \n",
    "    def decode(self, z:torch.Tensor, input_embeddings:torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Decodes z as a distribution over the vocabulary for each position in the sequence\"\"\"\n",
    "        batch_sz = z.size(0)\n",
    "        hidden = self.latent2hidden(z)\n",
    "        hidden = hidden.view(self.hidden_factor, batch_sz, self.hidden_dim)\n",
    "        output, _ = self.decoder_rnn(input_embeddings, hidden)\n",
    "        return F.softmax(self.outputs2vocab(output), dim=-1)\n",
    "        \n",
    "    def samplePrior(self, batch_sz:int) -> torch.Tensor:\n",
    "        \"\"\"Samples z from a unit multivariate Gaussian\"\"\"\n",
    "        return torch.randn(batch_sz, self.latent_dim)\n",
    "\n",
    "    def samplePosterior(self, mean:torch.Tensor, stdev:torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the approximate multivariate Gaussian posterior parameterized by\n",
    "        mean vector and diagonal covariance matrix.\n",
    "        \"\"\"\n",
    "        batch_sz = mean.size(0)\n",
    "        epsilon = self.samplePrior(batch_sz)\n",
    "        return mean + stdev * epsilon\n",
    "    \n",
    "    def generateSequences(self, z:torch.Tensor, max_len:int=150, greedy:bool=False) -> torch.Tensor:\n",
    "        \"\"\"Generates a batch of sequences from latent space encodings.\"\"\"\n",
    "        batch_sz = z.size(0)\n",
    "        sequences = torch.full([batch_sz, max_len], self.pad_idx, dtype=torch.long)\n",
    "        sequences[:, 0] = self.sos_idx\n",
    "        # define mask\n",
    "        \n",
    "        running_seqs = sequences.clone()\n",
    "        for s in range(max_len):\n",
    "            input_embeddings = self.embedding(sequences[:, :s+1])\n",
    "            logits = decode(z, input_embeddings)\n",
    "            # sample from softmax at sequence position - s\n",
    "            next_idxs = self._sample(logits[:,-1,:])\n",
    "            \n",
    "    def _sample(self, logits:torch.Tensor, greedy:bool=False) -> torch.Tensor:\n",
    "        \"\"\"Samples idxs from a softmax distribution\"\"\"\n",
    "        batch_sz = logits.size(0)\n",
    "        rand = torch.rand(batch_sz).repeat(self.vocab_sz, 1).T\n",
    "        cdf = logits.cumsum(-1)\n",
    "        sample = (rand > cdf).long().sum(-1)\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = MolecularVAE(len(vocabulary), 8, 64, 128, rnn_layers=2, bidirectional=True, dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 64])\n",
      "torch.Size([16, 98, 128])\n"
     ]
    }
   ],
   "source": [
    "logits, z, mean, logv = vae(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 98, 39])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(3, 4, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:, 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 2, 2, 2],\n",
       "        [2, 2, 2, 2],\n",
       "        [2, 2, 2, 2]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.full([3, 4], 2, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0],\n",
       "        [1, 0, 0, 0],\n",
       "        [1, 0, 0, 0]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0299, 0.0480, 0.0705, 0.1018, 0.1289, 0.1576, 0.1916, 0.2205, 0.2444,\n",
       "         0.2706, 0.2926, 0.3168, 0.3418, 0.3664, 0.3845, 0.4184, 0.4481, 0.4718,\n",
       "         0.4932, 0.5217, 0.5399, 0.5661, 0.5902, 0.6220, 0.6424, 0.6630, 0.6873,\n",
       "         0.7144, 0.7391, 0.7710, 0.7952, 0.8169, 0.8447, 0.8745, 0.9010, 0.9232,\n",
       "         0.9476, 0.9751, 1.0000],\n",
       "        [0.0261, 0.0569, 0.0866, 0.1138, 0.1423, 0.1651, 0.1965, 0.2209, 0.2497,\n",
       "         0.2720, 0.2973, 0.3233, 0.3495, 0.3687, 0.3982, 0.4154, 0.4415, 0.4682,\n",
       "         0.4888, 0.5126, 0.5340, 0.5593, 0.5778, 0.6063, 0.6290, 0.6498, 0.6809,\n",
       "         0.7063, 0.7305, 0.7610, 0.7875, 0.8109, 0.8339, 0.8670, 0.8891, 0.9333,\n",
       "         0.9529, 0.9747, 1.0000],\n",
       "        [0.0264, 0.0505, 0.0754, 0.0974, 0.1267, 0.1502, 0.1774, 0.1962, 0.2227,\n",
       "         0.2527, 0.2804, 0.3081, 0.3306, 0.3629, 0.3821, 0.4015, 0.4295, 0.4585,\n",
       "         0.4874, 0.5154, 0.5369, 0.5612, 0.5823, 0.6145, 0.6411, 0.6654, 0.6878,\n",
       "         0.7111, 0.7306, 0.7592, 0.7816, 0.8065, 0.8358, 0.8695, 0.8950, 0.9234,\n",
       "         0.9503, 0.9734, 1.0000],\n",
       "        [0.0263, 0.0489, 0.0722, 0.1022, 0.1332, 0.1591, 0.1847, 0.2077, 0.2383,\n",
       "         0.2680, 0.2904, 0.3202, 0.3406, 0.3712, 0.3890, 0.4125, 0.4362, 0.4713,\n",
       "         0.4940, 0.5243, 0.5506, 0.5780, 0.6020, 0.6323, 0.6611, 0.6817, 0.7077,\n",
       "         0.7357, 0.7523, 0.7823, 0.8068, 0.8284, 0.8513, 0.8783, 0.9001, 0.9237,\n",
       "         0.9465, 0.9732, 1.0000],\n",
       "        [0.0252, 0.0447, 0.0721, 0.1066, 0.1322, 0.1622, 0.1947, 0.2248, 0.2536,\n",
       "         0.2720, 0.2905, 0.3113, 0.3334, 0.3520, 0.3763, 0.4061, 0.4273, 0.4484,\n",
       "         0.4715, 0.4991, 0.5239, 0.5555, 0.5831, 0.6083, 0.6330, 0.6561, 0.6866,\n",
       "         0.7151, 0.7400, 0.7700, 0.7906, 0.8203, 0.8477, 0.8817, 0.9011, 0.9248,\n",
       "         0.9479, 0.9720, 1.0000],\n",
       "        [0.0332, 0.0534, 0.0764, 0.1057, 0.1329, 0.1597, 0.1812, 0.2035, 0.2287,\n",
       "         0.2500, 0.2768, 0.3065, 0.3258, 0.3502, 0.3753, 0.4052, 0.4295, 0.4527,\n",
       "         0.4790, 0.5008, 0.5241, 0.5523, 0.5808, 0.6059, 0.6253, 0.6508, 0.6829,\n",
       "         0.7187, 0.7391, 0.7699, 0.7922, 0.8162, 0.8471, 0.8733, 0.8995, 0.9234,\n",
       "         0.9485, 0.9766, 1.0000],\n",
       "        [0.0323, 0.0555, 0.0806, 0.1115, 0.1355, 0.1604, 0.1871, 0.2137, 0.2426,\n",
       "         0.2718, 0.2985, 0.3196, 0.3406, 0.3714, 0.3927, 0.4163, 0.4429, 0.4717,\n",
       "         0.4967, 0.5202, 0.5394, 0.5741, 0.5985, 0.6229, 0.6414, 0.6642, 0.6860,\n",
       "         0.7086, 0.7296, 0.7574, 0.7816, 0.8083, 0.8415, 0.8685, 0.8952, 0.9205,\n",
       "         0.9475, 0.9718, 1.0000],\n",
       "        [0.0378, 0.0583, 0.0807, 0.1165, 0.1430, 0.1682, 0.1964, 0.2224, 0.2496,\n",
       "         0.2705, 0.2914, 0.3148, 0.3409, 0.3681, 0.3892, 0.4171, 0.4409, 0.4615,\n",
       "         0.4847, 0.5131, 0.5411, 0.5686, 0.5958, 0.6190, 0.6387, 0.6619, 0.6879,\n",
       "         0.7180, 0.7404, 0.7788, 0.8020, 0.8207, 0.8468, 0.8785, 0.9057, 0.9281,\n",
       "         0.9471, 0.9730, 1.0000],\n",
       "        [0.0244, 0.0455, 0.0690, 0.1131, 0.1382, 0.1698, 0.2001, 0.2323, 0.2531,\n",
       "         0.2736, 0.2975, 0.3211, 0.3411, 0.3650, 0.3876, 0.4184, 0.4513, 0.4738,\n",
       "         0.4977, 0.5285, 0.5489, 0.5736, 0.6064, 0.6359, 0.6543, 0.6750, 0.7075,\n",
       "         0.7509, 0.7722, 0.7980, 0.8245, 0.8420, 0.8612, 0.8828, 0.9054, 0.9305,\n",
       "         0.9487, 0.9740, 1.0000],\n",
       "        [0.0365, 0.0592, 0.0816, 0.1111, 0.1451, 0.1705, 0.2012, 0.2250, 0.2500,\n",
       "         0.2695, 0.2882, 0.3124, 0.3369, 0.3597, 0.3827, 0.4117, 0.4374, 0.4597,\n",
       "         0.4794, 0.5048, 0.5275, 0.5540, 0.5780, 0.6018, 0.6305, 0.6580, 0.6800,\n",
       "         0.7069, 0.7281, 0.7558, 0.7820, 0.8091, 0.8317, 0.8697, 0.8908, 0.9190,\n",
       "         0.9452, 0.9735, 1.0000],\n",
       "        [0.0296, 0.0518, 0.0720, 0.1035, 0.1375, 0.1651, 0.1937, 0.2188, 0.2460,\n",
       "         0.2672, 0.2934, 0.3181, 0.3402, 0.3698, 0.3904, 0.4154, 0.4394, 0.4663,\n",
       "         0.4901, 0.5192, 0.5423, 0.5725, 0.6015, 0.6230, 0.6481, 0.6780, 0.7028,\n",
       "         0.7290, 0.7518, 0.7773, 0.8029, 0.8229, 0.8418, 0.8767, 0.9062, 0.9363,\n",
       "         0.9582, 0.9781, 1.0000],\n",
       "        [0.0296, 0.0528, 0.0811, 0.1149, 0.1509, 0.1748, 0.2039, 0.2299, 0.2609,\n",
       "         0.2833, 0.3041, 0.3355, 0.3568, 0.3860, 0.4038, 0.4299, 0.4573, 0.4829,\n",
       "         0.5029, 0.5327, 0.5565, 0.5847, 0.6134, 0.6452, 0.6672, 0.6897, 0.7076,\n",
       "         0.7444, 0.7664, 0.7908, 0.8172, 0.8377, 0.8627, 0.8884, 0.9117, 0.9327,\n",
       "         0.9561, 0.9787, 1.0000],\n",
       "        [0.0289, 0.0502, 0.0738, 0.1121, 0.1397, 0.1641, 0.1929, 0.2166, 0.2413,\n",
       "         0.2638, 0.2897, 0.3176, 0.3410, 0.3687, 0.3891, 0.4168, 0.4411, 0.4601,\n",
       "         0.4807, 0.5100, 0.5331, 0.5692, 0.5941, 0.6182, 0.6382, 0.6585, 0.6927,\n",
       "         0.7192, 0.7382, 0.7643, 0.7913, 0.8145, 0.8442, 0.8730, 0.8961, 0.9253,\n",
       "         0.9476, 0.9745, 1.0000],\n",
       "        [0.0302, 0.0514, 0.0713, 0.1068, 0.1378, 0.1644, 0.1960, 0.2163, 0.2443,\n",
       "         0.2626, 0.2899, 0.3127, 0.3330, 0.3569, 0.3806, 0.4057, 0.4275, 0.4563,\n",
       "         0.4782, 0.5089, 0.5375, 0.5668, 0.5953, 0.6247, 0.6403, 0.6686, 0.6906,\n",
       "         0.7258, 0.7524, 0.7785, 0.7999, 0.8203, 0.8456, 0.8719, 0.8994, 0.9253,\n",
       "         0.9515, 0.9755, 1.0000],\n",
       "        [0.0302, 0.0524, 0.0850, 0.1144, 0.1549, 0.1822, 0.2070, 0.2303, 0.2607,\n",
       "         0.2848, 0.3128, 0.3352, 0.3621, 0.3869, 0.4115, 0.4407, 0.4671, 0.4942,\n",
       "         0.5155, 0.5407, 0.5623, 0.5885, 0.6093, 0.6297, 0.6493, 0.6746, 0.6996,\n",
       "         0.7277, 0.7596, 0.7851, 0.8138, 0.8338, 0.8581, 0.8828, 0.9052, 0.9297,\n",
       "         0.9468, 0.9714, 1.0000],\n",
       "        [0.0296, 0.0523, 0.0811, 0.1122, 0.1391, 0.1658, 0.1934, 0.2180, 0.2417,\n",
       "         0.2679, 0.2881, 0.3184, 0.3493, 0.3719, 0.3914, 0.4181, 0.4391, 0.4638,\n",
       "         0.4858, 0.5152, 0.5391, 0.5698, 0.5917, 0.6182, 0.6371, 0.6587, 0.6813,\n",
       "         0.7134, 0.7362, 0.7661, 0.7924, 0.8190, 0.8439, 0.8763, 0.9001, 0.9242,\n",
       "         0.9447, 0.9703, 1.0000]], grad_fn=<CumsumBackward>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumsum = logits[:,-1,:].cumsum(-1)\n",
    "cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 39])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumsum.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3322, 0.9561, 0.7550, 0.2446, 0.5193, 0.7769, 0.0704, 0.3630, 0.4226,\n",
       "        0.7019, 0.5724, 0.7982, 0.3781, 0.4372, 0.2969, 0.6745])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = torch.rand(16)\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3322, 0.3322, 0.3322, 0.3322, 0.3322, 0.3322, 0.3322, 0.3322, 0.3322,\n",
       "         0.3322, 0.3322, 0.3322, 0.3322, 0.3322, 0.3322, 0.3322, 0.3322, 0.3322,\n",
       "         0.3322, 0.3322, 0.3322, 0.3322, 0.3322, 0.3322, 0.3322, 0.3322, 0.3322,\n",
       "         0.3322, 0.3322, 0.3322, 0.3322, 0.3322, 0.3322, 0.3322, 0.3322, 0.3322,\n",
       "         0.3322, 0.3322, 0.3322],\n",
       "        [0.9561, 0.9561, 0.9561, 0.9561, 0.9561, 0.9561, 0.9561, 0.9561, 0.9561,\n",
       "         0.9561, 0.9561, 0.9561, 0.9561, 0.9561, 0.9561, 0.9561, 0.9561, 0.9561,\n",
       "         0.9561, 0.9561, 0.9561, 0.9561, 0.9561, 0.9561, 0.9561, 0.9561, 0.9561,\n",
       "         0.9561, 0.9561, 0.9561, 0.9561, 0.9561, 0.9561, 0.9561, 0.9561, 0.9561,\n",
       "         0.9561, 0.9561, 0.9561],\n",
       "        [0.7550, 0.7550, 0.7550, 0.7550, 0.7550, 0.7550, 0.7550, 0.7550, 0.7550,\n",
       "         0.7550, 0.7550, 0.7550, 0.7550, 0.7550, 0.7550, 0.7550, 0.7550, 0.7550,\n",
       "         0.7550, 0.7550, 0.7550, 0.7550, 0.7550, 0.7550, 0.7550, 0.7550, 0.7550,\n",
       "         0.7550, 0.7550, 0.7550, 0.7550, 0.7550, 0.7550, 0.7550, 0.7550, 0.7550,\n",
       "         0.7550, 0.7550, 0.7550],\n",
       "        [0.2446, 0.2446, 0.2446, 0.2446, 0.2446, 0.2446, 0.2446, 0.2446, 0.2446,\n",
       "         0.2446, 0.2446, 0.2446, 0.2446, 0.2446, 0.2446, 0.2446, 0.2446, 0.2446,\n",
       "         0.2446, 0.2446, 0.2446, 0.2446, 0.2446, 0.2446, 0.2446, 0.2446, 0.2446,\n",
       "         0.2446, 0.2446, 0.2446, 0.2446, 0.2446, 0.2446, 0.2446, 0.2446, 0.2446,\n",
       "         0.2446, 0.2446, 0.2446],\n",
       "        [0.5193, 0.5193, 0.5193, 0.5193, 0.5193, 0.5193, 0.5193, 0.5193, 0.5193,\n",
       "         0.5193, 0.5193, 0.5193, 0.5193, 0.5193, 0.5193, 0.5193, 0.5193, 0.5193,\n",
       "         0.5193, 0.5193, 0.5193, 0.5193, 0.5193, 0.5193, 0.5193, 0.5193, 0.5193,\n",
       "         0.5193, 0.5193, 0.5193, 0.5193, 0.5193, 0.5193, 0.5193, 0.5193, 0.5193,\n",
       "         0.5193, 0.5193, 0.5193],\n",
       "        [0.7769, 0.7769, 0.7769, 0.7769, 0.7769, 0.7769, 0.7769, 0.7769, 0.7769,\n",
       "         0.7769, 0.7769, 0.7769, 0.7769, 0.7769, 0.7769, 0.7769, 0.7769, 0.7769,\n",
       "         0.7769, 0.7769, 0.7769, 0.7769, 0.7769, 0.7769, 0.7769, 0.7769, 0.7769,\n",
       "         0.7769, 0.7769, 0.7769, 0.7769, 0.7769, 0.7769, 0.7769, 0.7769, 0.7769,\n",
       "         0.7769, 0.7769, 0.7769],\n",
       "        [0.0704, 0.0704, 0.0704, 0.0704, 0.0704, 0.0704, 0.0704, 0.0704, 0.0704,\n",
       "         0.0704, 0.0704, 0.0704, 0.0704, 0.0704, 0.0704, 0.0704, 0.0704, 0.0704,\n",
       "         0.0704, 0.0704, 0.0704, 0.0704, 0.0704, 0.0704, 0.0704, 0.0704, 0.0704,\n",
       "         0.0704, 0.0704, 0.0704, 0.0704, 0.0704, 0.0704, 0.0704, 0.0704, 0.0704,\n",
       "         0.0704, 0.0704, 0.0704],\n",
       "        [0.3630, 0.3630, 0.3630, 0.3630, 0.3630, 0.3630, 0.3630, 0.3630, 0.3630,\n",
       "         0.3630, 0.3630, 0.3630, 0.3630, 0.3630, 0.3630, 0.3630, 0.3630, 0.3630,\n",
       "         0.3630, 0.3630, 0.3630, 0.3630, 0.3630, 0.3630, 0.3630, 0.3630, 0.3630,\n",
       "         0.3630, 0.3630, 0.3630, 0.3630, 0.3630, 0.3630, 0.3630, 0.3630, 0.3630,\n",
       "         0.3630, 0.3630, 0.3630],\n",
       "        [0.4226, 0.4226, 0.4226, 0.4226, 0.4226, 0.4226, 0.4226, 0.4226, 0.4226,\n",
       "         0.4226, 0.4226, 0.4226, 0.4226, 0.4226, 0.4226, 0.4226, 0.4226, 0.4226,\n",
       "         0.4226, 0.4226, 0.4226, 0.4226, 0.4226, 0.4226, 0.4226, 0.4226, 0.4226,\n",
       "         0.4226, 0.4226, 0.4226, 0.4226, 0.4226, 0.4226, 0.4226, 0.4226, 0.4226,\n",
       "         0.4226, 0.4226, 0.4226],\n",
       "        [0.7019, 0.7019, 0.7019, 0.7019, 0.7019, 0.7019, 0.7019, 0.7019, 0.7019,\n",
       "         0.7019, 0.7019, 0.7019, 0.7019, 0.7019, 0.7019, 0.7019, 0.7019, 0.7019,\n",
       "         0.7019, 0.7019, 0.7019, 0.7019, 0.7019, 0.7019, 0.7019, 0.7019, 0.7019,\n",
       "         0.7019, 0.7019, 0.7019, 0.7019, 0.7019, 0.7019, 0.7019, 0.7019, 0.7019,\n",
       "         0.7019, 0.7019, 0.7019],\n",
       "        [0.5724, 0.5724, 0.5724, 0.5724, 0.5724, 0.5724, 0.5724, 0.5724, 0.5724,\n",
       "         0.5724, 0.5724, 0.5724, 0.5724, 0.5724, 0.5724, 0.5724, 0.5724, 0.5724,\n",
       "         0.5724, 0.5724, 0.5724, 0.5724, 0.5724, 0.5724, 0.5724, 0.5724, 0.5724,\n",
       "         0.5724, 0.5724, 0.5724, 0.5724, 0.5724, 0.5724, 0.5724, 0.5724, 0.5724,\n",
       "         0.5724, 0.5724, 0.5724],\n",
       "        [0.7982, 0.7982, 0.7982, 0.7982, 0.7982, 0.7982, 0.7982, 0.7982, 0.7982,\n",
       "         0.7982, 0.7982, 0.7982, 0.7982, 0.7982, 0.7982, 0.7982, 0.7982, 0.7982,\n",
       "         0.7982, 0.7982, 0.7982, 0.7982, 0.7982, 0.7982, 0.7982, 0.7982, 0.7982,\n",
       "         0.7982, 0.7982, 0.7982, 0.7982, 0.7982, 0.7982, 0.7982, 0.7982, 0.7982,\n",
       "         0.7982, 0.7982, 0.7982],\n",
       "        [0.3781, 0.3781, 0.3781, 0.3781, 0.3781, 0.3781, 0.3781, 0.3781, 0.3781,\n",
       "         0.3781, 0.3781, 0.3781, 0.3781, 0.3781, 0.3781, 0.3781, 0.3781, 0.3781,\n",
       "         0.3781, 0.3781, 0.3781, 0.3781, 0.3781, 0.3781, 0.3781, 0.3781, 0.3781,\n",
       "         0.3781, 0.3781, 0.3781, 0.3781, 0.3781, 0.3781, 0.3781, 0.3781, 0.3781,\n",
       "         0.3781, 0.3781, 0.3781],\n",
       "        [0.4372, 0.4372, 0.4372, 0.4372, 0.4372, 0.4372, 0.4372, 0.4372, 0.4372,\n",
       "         0.4372, 0.4372, 0.4372, 0.4372, 0.4372, 0.4372, 0.4372, 0.4372, 0.4372,\n",
       "         0.4372, 0.4372, 0.4372, 0.4372, 0.4372, 0.4372, 0.4372, 0.4372, 0.4372,\n",
       "         0.4372, 0.4372, 0.4372, 0.4372, 0.4372, 0.4372, 0.4372, 0.4372, 0.4372,\n",
       "         0.4372, 0.4372, 0.4372],\n",
       "        [0.2969, 0.2969, 0.2969, 0.2969, 0.2969, 0.2969, 0.2969, 0.2969, 0.2969,\n",
       "         0.2969, 0.2969, 0.2969, 0.2969, 0.2969, 0.2969, 0.2969, 0.2969, 0.2969,\n",
       "         0.2969, 0.2969, 0.2969, 0.2969, 0.2969, 0.2969, 0.2969, 0.2969, 0.2969,\n",
       "         0.2969, 0.2969, 0.2969, 0.2969, 0.2969, 0.2969, 0.2969, 0.2969, 0.2969,\n",
       "         0.2969, 0.2969, 0.2969],\n",
       "        [0.6745, 0.6745, 0.6745, 0.6745, 0.6745, 0.6745, 0.6745, 0.6745, 0.6745,\n",
       "         0.6745, 0.6745, 0.6745, 0.6745, 0.6745, 0.6745, 0.6745, 0.6745, 0.6745,\n",
       "         0.6745, 0.6745, 0.6745, 0.6745, 0.6745, 0.6745, 0.6745, 0.6745, 0.6745,\n",
       "         0.6745, 0.6745, 0.6745, 0.6745, 0.6745, 0.6745, 0.6745, 0.6745, 0.6745,\n",
       "         0.6745, 0.6745, 0.6745]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsample = sample.repeat(39, 1).T\n",
    "rsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12, 37, 29,  9, 20, 30,  2, 13, 16, 27, 21, 30, 14, 17, 10, 26])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(rsample > cumsum).long().sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "385.875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
