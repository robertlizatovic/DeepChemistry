{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Molecular VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [18:37:51] Enabling RDKit 2019.09.1 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "# packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.utils.data as tud\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# project modules\n",
    "from utils import SMILESTokenizer, SMILESVocabulary, SMILESDataset, countTokens\n",
    "from models import MolecularVAE\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# chemistry\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "IPythonConsole.ipython_useSVG=True  #set this to False if you want PNGs instead of SVGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ChEMBL ID</th>\n",
       "      <th>Smiles</th>\n",
       "      <th>AlogP</th>\n",
       "      <th>QED Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL2333117</td>\n",
       "      <td>CC(C)Nc1c(C(N)=O)nnc2ccc(-c3cnn(C)c3)cc12</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL1189585</td>\n",
       "      <td>CC1C(=O)NC2=Nc3sc4c(c3CN21)CCCC4</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL4089494</td>\n",
       "      <td>CNC(=O)c1ccc(NC(=O)Nc2ccc(-c3nc(N4CCOCC4)c4ncc...</td>\n",
       "      <td>3.53</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL1189590</td>\n",
       "      <td>CN(C)c1nccc2c1nnn2Cc1ccccc1F</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3927722</td>\n",
       "      <td>Cc1noc(C)c1Cn1cc(NC(=O)Cc2ccco2)cn1</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ChEMBL ID                                             Smiles  AlogP  \\\n",
       "0  CHEMBL2333117          CC(C)Nc1c(C(N)=O)nnc2ccc(-c3cnn(C)c3)cc12   1.95   \n",
       "1  CHEMBL1189585                   CC1C(=O)NC2=Nc3sc4c(c3CN21)CCCC4   1.95   \n",
       "2  CHEMBL4089494  CNC(=O)c1ccc(NC(=O)Nc2ccc(-c3nc(N4CCOCC4)c4ncc...   3.53   \n",
       "3  CHEMBL1189590                       CN(C)c1nccc2c1nnn2Cc1ccccc1F   2.08   \n",
       "4  CHEMBL3927722                Cc1noc(C)c1Cn1cc(NC(=O)Cc2ccco2)cn1   2.31   \n",
       "\n",
       "   QED Weighted  \n",
       "0          0.77  \n",
       "1          0.78  \n",
       "2          0.40  \n",
       "3          0.73  \n",
       "4          0.78  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chembl = pd.read_csv(\"data/cleaned_dataset.csv\")\n",
    "chembl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation & encoding\n",
    "\n",
    "Steps to prepare VAE input data:\n",
    "1. SMILES tokenization (add start/end tokens)\n",
    "2. SMILES token encoding (convert to integer indecies) -> build a vocabulary\n",
    "3. Set up a SMILES dataset class (for feeding batches of data to the VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SMILESTokenizer()\n",
    "vocabulary = SMILESVocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary.build(chembl[\"Smiles\"], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save vocabulary\n",
    "# vocabulary.save(\"data/vocabulary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vocabulary\n",
    "vocabulary.load(\"data/vocabulary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze token frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C</td>\n",
       "      <td>12896279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(</td>\n",
       "      <td>7025189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>)</td>\n",
       "      <td>7025189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N</td>\n",
       "      <td>2682981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c</td>\n",
       "      <td>18431345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>4604265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>=</td>\n",
       "      <td>2904352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>O</td>\n",
       "      <td>4388805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>n</td>\n",
       "      <td>2124346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>3590237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-</td>\n",
       "      <td>576826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>1781615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>s</td>\n",
       "      <td>190809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>530543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>F</td>\n",
       "      <td>577815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>o</td>\n",
       "      <td>190476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Cl</td>\n",
       "      <td>340814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[</td>\n",
       "      <td>1253085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@</td>\n",
       "      <td>1286887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>H</td>\n",
       "      <td>936158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>]</td>\n",
       "      <td>1253085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>S</td>\n",
       "      <td>418728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5</td>\n",
       "      <td>98750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>#</td>\n",
       "      <td>109078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Br</td>\n",
       "      <td>70038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>/</td>\n",
       "      <td>345947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>\\</td>\n",
       "      <td>78135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6</td>\n",
       "      <td>11356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7</td>\n",
       "      <td>1131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>P</td>\n",
       "      <td>20234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>p</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>+</td>\n",
       "      <td>102796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>I</td>\n",
       "      <td>9080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>%</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   token       cnt\n",
       "0      C  12896279\n",
       "1      (   7025189\n",
       "2      )   7025189\n",
       "3      N   2682981\n",
       "4      c  18431345\n",
       "5      1   4604265\n",
       "6      =   2904352\n",
       "7      O   4388805\n",
       "8      n   2124346\n",
       "9      2   3590237\n",
       "10     -    576826\n",
       "11     3   1781615\n",
       "12     s    190809\n",
       "13     4    530543\n",
       "14     F    577815\n",
       "15     o    190476\n",
       "16    Cl    340814\n",
       "17     [   1253085\n",
       "18     @   1286887\n",
       "19     H    936158\n",
       "20     ]   1253085\n",
       "21     S    418728\n",
       "22     5     98750\n",
       "23     #    109078\n",
       "24    Br     70038\n",
       "25     /    345947\n",
       "26     \\     78135\n",
       "27     6     11356\n",
       "28     8       516\n",
       "29     7      1131\n",
       "30     P     20234\n",
       "31     p        17\n",
       "32     +    102796\n",
       "33     I      9080\n",
       "34     9        17\n",
       "35     %         8\n",
       "36     0         4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_cnts = countTokens(chembl[\"Smiles\"], tokenizer)\n",
    "token_cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot token distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "_ = sns.barplot(data=token_cnts.sort_values(\"cnt\", ascending=False), x=\"token\", y=\"cnt\", color=\"steelblue\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/validation/test split\n",
    "\n",
    "A randomly selected validation and test set will be put aside for monitoring the network training process itself (validation set) and evaluating the trained models (test set).\n",
    "\n",
    "Models will be evaluated in terms of the mean **E**vidence **L**ower **BO**und (**ELBO**) of the test set sequences (SMILES). The higher the better. This is equivalent to to saying that the loss function (which is the negative ELBO) should be as low as possible: \n",
    "\n",
    "$$L(S;\\theta)=-ELBO(S;\\theta)=KL(S;\\theta) + NLL(S;\\theta)$$\n",
    "\n",
    "where S is the set of SMILES strings in the test set, $KL(S;\\theta)$ is the mean KL divergence of the test set (obtained from the encoder of the VAE), and $NLL(S;\\theta)$ is the mean neg. log-likelihood of the test set (obtained from the decoder of the VAE). See bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(chembl, test_size=0.1, random_state=42)\n",
    "train, validation = train_test_split(train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1292494, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a dataset\n",
    "\n",
    "A dataset is created for each split so batches of samples can easily be fed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = SMILESDataset(train[\"Smiles\"], vocabulary, tokenizer)\n",
    "val_ds = SMILESDataset(validation[\"Smiles\"], vocabulary, tokenizer)\n",
    "test_ds = SMILESDataset(test[\"Smiles\"], vocabulary, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE models\n",
    "\n",
    "The variational autoencoder consists of the following components:\n",
    "1. SMILES encoder: maps input sequences to a latent vector z.\n",
    "2. Decoder: decodes a latent vector z into a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = MolecularVAE(len(vocabulary), 8, 64, 128, vocabulary.getStartIdx(), vocabulary.getEndIdx(), \n",
    "                   vocabulary.getPadIdx(), rnn_layers=2, bidirectional=True, dropout=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The loss function\n",
    "\n",
    "The loss function used for training the VAE is the negative of the **E**vidence **L**ower **BO**und (**ELBO**). ELBO consists of 2 terms, and represents the lower bound on the marginal log-likelihood of the data:\n",
    "\n",
    "$$ELBO(S;\\theta) = E_{q_{\\theta}(Z|S)}[\\log{p_{\\theta}(S|Z)}] - KL(q_{\\theta}(Z|S) \\lVert p(Z)) \\leq \\log{p(S)}$$\n",
    "\n",
    "Here, $Z$ is the latent variable, $S$ is the observed SMILES data (evidence), $q_{\\theta}(Z|S)$ (encoder output) is the approximation to the true posterior distribution - $p(Z|S)$, $p_{\\theta}(S|Z)$ is the conditional distribution over $S$ given $Z$ (decoder output), $p(Z)$ is the prior ditribution of the latent variable $Z$, and $p(S)$ is the marginal distribution of the data.\n",
    "\n",
    "The first term is estimated using a single sample drawn from the approximate posterior, whereas the KL term is computed analytically in closed form. The goal of the optimization is to maximize the lower bound, and this can be achieved by minimizing its negative (the loss function):\n",
    "\n",
    "$$L(S;\\theta) = -\\log{p_{\\theta}(S|Z)} + KL(q_{\\theta}(Z|S) \\lVert p(Z))$$\n",
    "\n",
    "Here also the expectation term was replaced by the neg. log-likelihood of $S$ given a single $Z$ drawn from $q_{\\theta}(Z|S)$ as explained above. For a minibatch of samples - $S$, the loss function then becomes:\n",
    "\n",
    "$$L(S;\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m} \\log{p_{\\theta}(S_i|Z_i)} + \\frac{1}{m}\\sum_{i=1}^{m} KL(q_{\\theta}(Z_i|S_i) \\lVert p(Z))=NLL + KL$$\n",
    "\n",
    "Here, $i$ is a particular example from a batch of $m$ samples.\n",
    "\n",
    "Because $q_{\\theta}(Z_i|S_i)$ and $p(Z)$ are both chosen to be diagonal multivariate gaussians and in particular $p(Z) \\sim \\mathcal{N}(0, I)$, the KL divergence term can be computed analytically in closed form:\n",
    "\n",
    "$$KL(q_{\\theta}(Z_i|S_i) \\lVert p(Z)) = \\frac{1}{2}\\left(\\lVert\\mu_q\\lVert^2 - k + tr\\{\\Sigma_q\\} - \\log{|\\Sigma_q|}\\right)=\\frac{1}{2}\\left(\\sum_{j=1}^{k} \\mu_j^2 - k + \\sum_{j=1}^{k} \\nu_j - \\sum_{j=1}^{k} \\log{\\nu_j}\\right)=\\frac{1}{2}\\sum_{j=1}^{k}\\left(\\mu_j^2 - 1 + \\nu_j - \\log{\\nu_j}\\right)$$\n",
    "\n",
    "where $\\mu_j$ and $\\nu_j$ are the $j^{th}$ components of the mean and variance vectors of $q_{\\theta}(Z_i|S_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def compute_loss(model:nn.Module, batch:torch.Tensor) -> tuple:\n",
    "    \"\"\"Computes the NLL and KL loss for a batch of sequences\"\"\"\n",
    "    batch_sz = batch.size(0)\n",
    "    # prepare targets (input shifted 1 step to the left and padded with pad idx)\n",
    "    targets = batch.roll(shifts=-1, dims=1)\n",
    "    targets[:, -1] = model.pad_idx\n",
    "    # prepare predictions\n",
    "    logp, _, mean, logv = model(batch)\n",
    "    preds = logp.permute(0, 2, 1)\n",
    "    # compute NLL (neg. log likelihood) loss for the target sequence (ignore padding idx)\n",
    "    nll_loss = F.nll_loss(preds, targets, ignore_index=model.pad_idx, reduction=\"sum\") / batch_sz\n",
    "    # compute KL loss\n",
    "    kl_loss = -0.5 * torch.sum(1 + logv - mean.pow(2) - logv.exp()) / batch_sz\n",
    "    # compute total loss (averged within a batch)\n",
    "    return (nll_loss, kl_loss)\n",
    "\n",
    "def annealing_func(nll_loss:torch.Tensor, kl_loss:torch.Tensor, s:int, m:int, w:int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Controls the final loss function through an adjustable KL weight\n",
    "    -------------------\n",
    "    s - iteration step\n",
    "    m - midpoint (iteration step at which the KL weight is 0.5)\n",
    "    w - width (smoothnes of the sigmoid annealing function around the midpoint)\n",
    "    \"\"\"\n",
    "    kl_weight = 1 / (1 + np.exp(-(s - m)/w))\n",
    "    return nll_loss + kl_weight * kl_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1\n",
    "BATCH_SZ = 128\n",
    "KLW_MIDPOINT = 4000\n",
    "KLW_WIDTH = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training batch: 0, ELBO: 179.66, NLL: 179.65, KL div: 0.52\n",
      "Training batch: 100, ELBO: 39.01, NLL: 38.76, KL div: 12.90\n",
      "Training batch: 200, ELBO: 5.01, NLL: 4.89, KL div: 5.70\n",
      "Training batch: 300, ELBO: 1.48, NLL: 1.39, KL div: 3.52\n",
      "Training batch: 400, ELBO: 0.84, NLL: 0.78, KL div: 2.08\n",
      "Training batch: 500, ELBO: 0.38, NLL: 0.34, KL div: 1.42\n",
      "Training batch: 600, ELBO: 0.27, NLL: 0.24, KL div: 0.92\n",
      "Training batch: 700, ELBO: 0.45, NLL: 0.43, KL div: 0.63\n",
      "Training batch: 800, ELBO: 0.21, NLL: 0.19, KL div: 0.41\n",
      "Training batch: 900, ELBO: 0.13, NLL: 0.12, KL div: 0.21\n",
      "Training batch: 1000, ELBO: 0.15, NLL: 0.15, KL div: 0.10\n",
      "Training batch: 1100, ELBO: 0.08, NLL: 0.08, KL div: 0.05\n",
      "Training batch: 1200, ELBO: 0.09, NLL: 0.09, KL div: 0.03\n",
      "Training batch: 1300, ELBO: 0.08, NLL: 0.08, KL div: 0.01\n",
      "Training batch: 1400, ELBO: 0.04, NLL: 0.04, KL div: 0.01\n",
      "Training batch: 1500, ELBO: 0.17, NLL: 0.17, KL div: 0.00\n",
      "Training batch: 1600, ELBO: 0.03, NLL: 0.03, KL div: 0.00\n",
      "Training batch: 1700, ELBO: 0.03, NLL: 0.03, KL div: 0.00\n",
      "Training batch: 1800, ELBO: 0.03, NLL: 0.03, KL div: 0.01\n",
      "Training batch: 1900, ELBO: 0.02, NLL: 0.02, KL div: 0.00\n",
      "Training batch: 2000, ELBO: 0.02, NLL: 0.02, KL div: 0.04\n",
      "Training batch: 2100, ELBO: 0.02, NLL: 0.02, KL div: 0.02\n",
      "Training batch: 2200, ELBO: 0.07, NLL: 0.02, KL div: 0.35\n",
      "Training batch: 2300, ELBO: 0.03, NLL: 0.02, KL div: 0.04\n",
      "Training batch: 2400, ELBO: 0.02, NLL: 0.02, KL div: 0.01\n",
      "Training batch: 2500, ELBO: 0.02, NLL: 0.01, KL div: 0.03\n",
      "Training batch: 2600, ELBO: 0.02, NLL: 0.01, KL div: 0.03\n",
      "Training batch: 2700, ELBO: 0.05, NLL: 0.04, KL div: 0.01\n",
      "Training batch: 2800, ELBO: 0.01, NLL: 0.01, KL div: 0.00\n",
      "Training batch: 2900, ELBO: 0.01, NLL: 0.01, KL div: 0.00\n",
      "Training batch: 3000, ELBO: 0.01, NLL: 0.01, KL div: 0.00\n",
      "Training batch: 3100, ELBO: 0.01, NLL: 0.01, KL div: 0.00\n",
      "Training batch: 3200, ELBO: 0.01, NLL: 0.01, KL div: 0.00\n",
      "Training batch: 3300, ELBO: 0.01, NLL: 0.01, KL div: 0.00\n",
      "Training batch: 3400, ELBO: 0.01, NLL: 0.01, KL div: 0.00\n",
      "Training batch: 3500, ELBO: 0.01, NLL: 0.01, KL div: 0.00\n",
      "Training batch: 3600, ELBO: 0.01, NLL: 0.01, KL div: 0.00\n",
      "Training batch: 3700, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 3800, ELBO: 0.01, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 3900, ELBO: 0.01, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 4000, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 4100, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 4200, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 4300, ELBO: 0.01, NLL: 0.01, KL div: 0.00\n",
      "Training batch: 4400, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 4500, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 4600, ELBO: 0.01, NLL: 0.01, KL div: 0.00\n",
      "Training batch: 4700, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 4800, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 4900, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 5000, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 5100, ELBO: 0.01, NLL: 0.01, KL div: 0.00\n",
      "Training batch: 5200, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 5300, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 5400, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 5500, ELBO: 0.01, NLL: 0.01, KL div: 0.00\n",
      "Training batch: 5600, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 5700, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 5800, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 5900, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 6000, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 6100, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 6200, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 6300, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 6400, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 6500, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 6600, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 6700, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 6800, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 6900, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 7000, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 7100, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 7200, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 7300, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 7400, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 7500, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 7600, ELBO: 0.10, NLL: 0.10, KL div: 0.00\n",
      "Training batch: 7700, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 7800, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 7900, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 8000, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 8100, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 8200, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 8300, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 8400, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 8500, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 8600, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 8700, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 8800, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 8900, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 9000, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 9100, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 9200, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 9300, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 9400, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 9500, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 9600, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 9700, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 9800, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 9900, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n",
      "Training batch: 10000, ELBO: 0.00, NLL: 0.00, KL div: 0.00\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-2aa267bd0e00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mval_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"NLL\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mval_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"KL\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkl_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mval_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_scores\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     print(\"Epoch %i validation scores: ELBO: %.2f, NLL: %.2f, KL div: %.2f\" % (e, \n\u001b[1;32m     36\u001b[0m                                    val_mean[\"ELBO\"], val_mean[\"NLL\"], val_mean[\"KL\"])) \n",
      "\u001b[0;32m<ipython-input-30-2aa267bd0e00>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mval_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"NLL\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mval_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"KL\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkl_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mval_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_scores\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     print(\"Epoch %i validation scores: ELBO: %.2f, NLL: %.2f, KL div: %.2f\" % (e, \n\u001b[1;32m     36\u001b[0m                                    val_mean[\"ELBO\"], val_mean[\"NLL\"], val_mean[\"KL\"])) \n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "step = 0\n",
    "for e in range(1, NUM_EPOCHS+1):\n",
    "    print(\"Epoch: %i\" %e)\n",
    "    # train model on the training set\n",
    "    dataloader = tud.DataLoader(train_ds, batch_size=BATCH_SZ, shuffle=True, collate_fn=train_ds.getCollateFn())\n",
    "    # enable dropout\n",
    "    vae.train()\n",
    "#     train_scores = {\"ELBO\":[], \"NLL\":[], \"KL\":[]}\n",
    "    for b, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        # compute loss\n",
    "        nll_loss, kl_loss = compute_loss(vae, batch)\n",
    "        loss = annealing_func(nll_loss, kl_loss, step, KLW_MIDPOINT, KLW_WIDTH)\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "        # print training progress\n",
    "        if b % 100 == 0:\n",
    "            print(\"Training batch: %i, ELBO: %.4f, NLL: %.4f, KL div: %.4f\" % (b, \n",
    "                                   loss.item(), nll_loss.item(), kl_loss.item()))\n",
    "    # evaluate on the validation set\n",
    "    dataloader = tud.DataLoader(val_ds, batch_size=BATCH_SZ, shuffle=False, collate_fn=val_ds.getCollateFn())\n",
    "    # disable dropout\n",
    "    vae.eval()\n",
    "    val_scores = {\"ELBO\":[], \"NLL\":[], \"KL\":[]}\n",
    "    for b, batch in enumerate(dataloader):\n",
    "        nll_loss, kl_loss = compute_loss(vae, batch)\n",
    "        loss = nll_loss + kl_loss\n",
    "        val_scores[\"ELBO\"].append(loss.item())\n",
    "        val_scores[\"NLL\"].append(nll_loss.item())\n",
    "        val_scores[\"KL\"].append(kl_loss.item())\n",
    "    val_mean = {k:np.mean(v) for k, v in val_scores.items()}\n",
    "    print(\"Epoch %i validation scores: ELBO: %.4f, NLL: %.4f, KL div: %.4f\" % (e, \n",
    "                                   val_mean[\"ELBO\"], val_mean[\"NLL\"], val_mean[\"KL\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 validation scores: ELBO: 0.0004, NLL: 0.0004, KL div: 0.0000\n"
     ]
    }
   ],
   "source": [
    "val_mean = {k:np.mean(v) for k, v in val_scores.items()}\n",
    "print(\"Epoch %i validation scores: ELBO: %.4f, NLL: %.4f, KL div: %.4f\" % (e, \n",
    "                               val_mean[\"ELBO\"], val_mean[\"NLL\"], val_mean[\"KL\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(vae.state_dict(), \"saved_models/gru_vae.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MolecularVAE(\n",
       "  (embedding): Embedding(40, 8, padding_idx=0)\n",
       "  (encoder_rnn): GRU(8, 64, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "  (hidden2mean): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (hidden2logv): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (latent2hidden): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (decoder_rnn): GRU(8, 64, num_layers=2, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "  (outputs2vocab): Linear(in_features=128, out_features=40, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.load_state_dict(torch.load(\"saved_models/gru_vae.pt\"))\n",
    "# set to inference mode\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples = vae.generateSequences(n=32, max_len=150, greedy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C(C',\n",
       " '',\n",
       " 'Cl))CC7P)C11n(CClnn==BrBrcBrBrN=====BrBr==PBr)====)==P=P============PBr==P===PBrBrBr5==PBr==Br=P==PBrBrBrBrBr=Br=P=Br5=PBr5Br=======Br=PBr==PPPBrBrBr)==========Br=P====BrBr)Br5)BrBr5PP',\n",
       " 'CBrBr))C',\n",
       " 'SNNBr5)CBr)CO',\n",
       " '5O]==Br)SN)c1)SC]',\n",
       " '==N(N=BrCScc1Br)[O',\n",
       " '8))==C==CBr=)N=c1CCNN=PBr5',\n",
       " '==N)Br=C)NCCcCC=)cN=N8]]CN))C]N8]NNC]CNNNNO]NCC8@]NON3N8]3N3NO]NCCONNO]]N3]]NCC]N3N8]]NCCNNCONNNCI]NO3]NO]]OINNCCCNN]NCI]OC]3(CC]]NNC(NCNCINNNc/',\n",
       " ')Br)[N)=[)NN===Nccccc=====P=N==N=N=N==N==N==N==N===N=N====N=N=N=N]==N==N==N==N===N=====N======N]=N=N=N===N======N====N=N=N=N===N==N=N=N==N=N',\n",
       " '=O1Br5N)BrN))N)Br=PP]SN)))))S)Br)Br[Br)Br)S)S))Br))Br)Br))))Br))))BrBr))Br))[)BrBr)Br)Br)Br))Br)))cBrBr))Br)BrBr)=))S)Br)Br[)Br)S)Br)S)Br))SBr)Br5))cc5)Br))[)Br)Br)Br)Br)Br)Br)[Br)Br)=BrS5)BrBr',\n",
       " 'Br)N',\n",
       " 'OCCNsBrNs)NN=#',\n",
       " 'N3nC\\\\s)\\\\sBrBr)BrBr)CCBr==SNNNN[c5=Br=P=Br==Br=PPPBr[N[BrClCl=PPBr=P=N=P)=N==N=NN=[NPNNNN3S=P=)N1P[SBr===Br=BrP==[N[N=N[Br[=SS=BrSN=ClPBr5PBrNBr=PPN/=N[BrClPP===N=[Br5BrBr5SSBrcc',\n",
       " 'Css3C\\\\sCSss#ss[NNBrccccccc[SBr)N)Br)Br)Br)[N)Br)Br)N)Br)BrBr)N)[Br)BrN)Br)Br)N)NNBr)BrccBrNSBrBr)NN)Br)Br)Br)Br)Br)Br)Br)[)NN)Br)Br)Br)BrBr)S)BrBr)N)Br)N)Br)Br))N)BrN)BrN)BrN)Br)Br)NNNNS',\n",
       " '3\\\\ss(CS(C\\\\sBr\\\\\\\\S5)SSSBrcccc4',\n",
       " 'sCC\\\\Br(CSSCCsCC#PP]8]SSSSSSSBrcccccccccccccccccccccccccccccccccccccccc5ccccccccccccccccccccccccccccccccccccccccccBrccccccccccccccBrcc3BrBrccccccccCNC8SSSSS',\n",
       " 's(CCSBr(SBr4Cl\\\\((S))SSSCCCCCS\\\\SN\\\\SS[SSSSSSSSN)SNSSN\\\\SSN\\\\[SS\\\\SSSSS)\\\\\\\\[SN\\\\N\\\\S\\\\cSS[\\\\ScSSSSSSSSN\\\\\\\\[SSS\\\\SSSSSSSS[SNNSN\\\\\\\\S\\\\c3SSSSSSSSSSN\\\\SSSSSSSNSN\\\\\\\\S/SSnS/4Cl',\n",
       " 'C##=ISBr8SSBr4Br5',\n",
       " '(SSBrSClSSCSS(SSSBrSSSSSSSSSnn44444SC444444N4444PI444N44N44N44N4ClN4I44ClClN3444N3444444N44444444444N44]48C48]444N4344444444N884444444344N3(ClN@844SSSS5CSSS',\n",
       " 'Br8]SSSCSSC',\n",
       " 'SSSNnSSnSSSBrCCSSS1s[SSSSS55SSSS/SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSCCCC',\n",
       " 'CSSS5nS5n\\\\sSSSC44555snCCC1',\n",
       " 'SnnS',\n",
       " 'S5)CS5C',\n",
       " 'CCl',\n",
       " '5Sc51',\n",
       " '#11',\n",
       " '/SSSSBr5Br5C\\\\CSS5SSClCl',\n",
       " '1CC1C111',\n",
       " 'Sc5S5SBrS)55SCC',\n",
       " 'CNNCNC1C11']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_mols = [tokenizer.untokenize(vocabulary.decode(s)) for s in samples.tolist()]\n",
    "gen_mols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O=C(/C=C/c1ccc2c(c1)C(=O)N(Cc1ccccc1)C1(CCNCC1)O2)NO'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try sampling from a posterior of a sample from the test set\n",
    "tokenizer.untokenize(vocabulary.decode(test_ds[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "logp, z, mean, logv = vae(test_ds[0].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "bool value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-77847910b609>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msamples2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerateSequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgreedy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msamples2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/DeepChemistry/models.py\u001b[0m in \u001b[0;36mgenerateSequences\u001b[0;34m(self, n, z, max_len, greedy)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;34m\"\"\"Generates a batch of sequences from latent space encodings.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# if z is not given, sample from prior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplePrior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mbatch_sz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: bool value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "samples2 = vae.generateSequences(z=z, max_len=150, greedy=False)\n",
    "samples2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand = torch.rand(1).repeat(40)\n",
    "rand.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3751, 0.3751, 0.3751, 0.3751, 0.3751, 0.3751, 0.3751, 0.3751, 0.3751,\n",
       "        0.3751, 0.3751, 0.3751, 0.3751, 0.3751, 0.3751, 0.3751, 0.3751, 0.3751,\n",
       "        0.3751, 0.3751, 0.3751, 0.3751, 0.3751, 0.3751, 0.3751, 0.3751, 0.3751,\n",
       "        0.3751, 0.3751, 0.3751, 0.3751, 0.3751, 0.3751, 0.3751, 0.3751, 0.3751,\n",
       "        0.3751, 0.3751, 0.3751, 0.3751])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.5430e-10, 9.6822e-10, 4.6560e-08, 4.6630e-08, 4.7742e-08, 1.1781e-07,\n",
       "        1.7558e-07, 2.3136e-07, 2.3652e-07, 2.3656e-07, 2.3768e-07, 2.8132e-07,\n",
       "        3.2993e-07, 3.2996e-07, 3.3000e-07, 3.3321e-07, 3.3329e-07, 4.4612e-07,\n",
       "        8.4286e-07, 8.4300e-07, 1.2537e-06, 1.3216e-06, 1.3216e-06, 1.3627e-06,\n",
       "        1.3824e-06, 1.3862e-06, 1.5509e-06, 1.6564e-06, 1.6565e-06, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
       "       grad_fn=<CumsumBackward>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumsum = logp.exp()[0,0,:].cumsum(-1)\n",
    "cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(29)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(rand > cumsum).long().sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "362.875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
