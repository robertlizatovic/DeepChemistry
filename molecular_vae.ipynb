{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Molecular VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [18:20:00] Enabling RDKit 2019.09.1 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "# packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.utils.data as tud\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# project modules\n",
    "from utils import SMILESTokenizer, SMILESVocabulary, SMILESDataset, countTokens\n",
    "from models import MolecularVAE\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# chemistry\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "IPythonConsole.ipython_useSVG=True  #set this to False if you want PNGs instead of SVGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ChEMBL ID</th>\n",
       "      <th>Smiles</th>\n",
       "      <th>AlogP</th>\n",
       "      <th>QED Weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CHEMBL2333117</td>\n",
       "      <td>CC(C)Nc1c(C(N)=O)nnc2ccc(-c3cnn(C)c3)cc12</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CHEMBL1189585</td>\n",
       "      <td>CC1C(=O)NC2=Nc3sc4c(c3CN21)CCCC4</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEMBL4089494</td>\n",
       "      <td>CNC(=O)c1ccc(NC(=O)Nc2ccc(-c3nc(N4CCOCC4)c4ncc...</td>\n",
       "      <td>3.53</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHEMBL1189590</td>\n",
       "      <td>CN(C)c1nccc2c1nnn2Cc1ccccc1F</td>\n",
       "      <td>2.08</td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHEMBL3927722</td>\n",
       "      <td>Cc1noc(C)c1Cn1cc(NC(=O)Cc2ccco2)cn1</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ChEMBL ID                                             Smiles  AlogP  \\\n",
       "0  CHEMBL2333117          CC(C)Nc1c(C(N)=O)nnc2ccc(-c3cnn(C)c3)cc12   1.95   \n",
       "1  CHEMBL1189585                   CC1C(=O)NC2=Nc3sc4c(c3CN21)CCCC4   1.95   \n",
       "2  CHEMBL4089494  CNC(=O)c1ccc(NC(=O)Nc2ccc(-c3nc(N4CCOCC4)c4ncc...   3.53   \n",
       "3  CHEMBL1189590                       CN(C)c1nccc2c1nnn2Cc1ccccc1F   2.08   \n",
       "4  CHEMBL3927722                Cc1noc(C)c1Cn1cc(NC(=O)Cc2ccco2)cn1   2.31   \n",
       "\n",
       "   QED Weighted  \n",
       "0          0.77  \n",
       "1          0.78  \n",
       "2          0.40  \n",
       "3          0.73  \n",
       "4          0.78  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chembl = pd.read_csv(\"data/cleaned_dataset.csv\")\n",
    "chembl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation & encoding\n",
    "\n",
    "Steps to prepare VAE input data:\n",
    "1. SMILES tokenization (add start/end tokens)\n",
    "2. SMILES token encoding (convert to integer indecies) -> build a vocabulary\n",
    "3. Set up a SMILES dataset class (for feeding batches of data to the VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SMILESTokenizer()\n",
    "vocabulary = SMILESVocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary.build(chembl[\"Smiles\"], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save vocabulary\n",
    "# vocabulary.save(\"data/vocabulary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vocabulary\n",
    "vocabulary.load(\"data/vocabulary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze token frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C</td>\n",
       "      <td>12896279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(</td>\n",
       "      <td>7025189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>)</td>\n",
       "      <td>7025189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N</td>\n",
       "      <td>2682981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c</td>\n",
       "      <td>18431345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>4604265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>=</td>\n",
       "      <td>2904352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>O</td>\n",
       "      <td>4388805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>n</td>\n",
       "      <td>2124346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>3590237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-</td>\n",
       "      <td>576826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>1781615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>s</td>\n",
       "      <td>190809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>530543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>F</td>\n",
       "      <td>577815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>o</td>\n",
       "      <td>190476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Cl</td>\n",
       "      <td>340814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[</td>\n",
       "      <td>1253085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@</td>\n",
       "      <td>1286887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>H</td>\n",
       "      <td>936158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>]</td>\n",
       "      <td>1253085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>S</td>\n",
       "      <td>418728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>5</td>\n",
       "      <td>98750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>#</td>\n",
       "      <td>109078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Br</td>\n",
       "      <td>70038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>/</td>\n",
       "      <td>345947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>\\</td>\n",
       "      <td>78135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>6</td>\n",
       "      <td>11356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>7</td>\n",
       "      <td>1131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>P</td>\n",
       "      <td>20234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>p</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>+</td>\n",
       "      <td>102796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>I</td>\n",
       "      <td>9080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>%</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   token       cnt\n",
       "0      C  12896279\n",
       "1      (   7025189\n",
       "2      )   7025189\n",
       "3      N   2682981\n",
       "4      c  18431345\n",
       "5      1   4604265\n",
       "6      =   2904352\n",
       "7      O   4388805\n",
       "8      n   2124346\n",
       "9      2   3590237\n",
       "10     -    576826\n",
       "11     3   1781615\n",
       "12     s    190809\n",
       "13     4    530543\n",
       "14     F    577815\n",
       "15     o    190476\n",
       "16    Cl    340814\n",
       "17     [   1253085\n",
       "18     @   1286887\n",
       "19     H    936158\n",
       "20     ]   1253085\n",
       "21     S    418728\n",
       "22     5     98750\n",
       "23     #    109078\n",
       "24    Br     70038\n",
       "25     /    345947\n",
       "26     \\     78135\n",
       "27     6     11356\n",
       "28     8       516\n",
       "29     7      1131\n",
       "30     P     20234\n",
       "31     p        17\n",
       "32     +    102796\n",
       "33     I      9080\n",
       "34     9        17\n",
       "35     %         8\n",
       "36     0         4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_cnts = countTokens(chembl[\"Smiles\"], tokenizer)\n",
    "token_cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot token distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "_ = sns.barplot(data=token_cnts.sort_values(\"cnt\", ascending=False), x=\"token\", y=\"cnt\", color=\"steelblue\", ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/validation/test split\n",
    "\n",
    "A randomly selected validation and test set will be put aside for monitoring the network training process itself (validation set) and evaluating the trained models (test set).\n",
    "\n",
    "Models will be evaluated in terms of the mean **E**vidence **L**ower **BO**und (**ELBO**) of the test set sequences (SMILES). The higher the better. This is equivalent to to saying that the loss function (which is the negative ELBO) should be as low as possible: \n",
    "\n",
    "$$L(S;\\theta)=-ELBO(S;\\theta)=KL(S;\\theta) + NLL(S;\\theta)$$\n",
    "\n",
    "where S is the set of SMILES strings in the test set, $KL(S;\\theta)$ is the mean KL divergence of the test set (obtained from the encoder of the VAE), and $NLL(S;\\theta)$ is the mean neg. log-likelihood of the test set (obtained from the decoder of the VAE). See bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(chembl, test_size=0.1, random_state=42)\n",
    "train, validation = train_test_split(train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1292494, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a dataset\n",
    "\n",
    "A dataset is created for each split so batches of samples can easily be fed to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = SMILESDataset(train[\"Smiles\"], vocabulary, tokenizer)\n",
    "val_ds = SMILESDataset(validation[\"Smiles\"], vocabulary, tokenizer)\n",
    "test_ds = SMILESDataset(test[\"Smiles\"], vocabulary, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE models\n",
    "\n",
    "The variational autoencoder consists of the following components:\n",
    "1. SMILES encoder: maps input sequences to a latent vector z.\n",
    "2. Decoder: decodes a latent vector z into a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = MolecularVAE(len(vocabulary), 16, 128, 128, vocabulary.getStartIdx(), vocabulary.getEndIdx(), vocabulary.getPadIdx(), \n",
    "                   rnn_layers=2, bidirectional=True, token_dropout=0.5, embedding_dropout=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The loss function\n",
    "\n",
    "The loss function used for training the VAE is the negative of the **E**vidence **L**ower **BO**und (**ELBO**). ELBO consists of 2 terms, and represents the lower bound on the marginal log-likelihood of the data:\n",
    "\n",
    "$$ELBO(S;\\theta) = E_{q_{\\theta}(Z|S)}[\\log{p_{\\theta}(S|Z)}] - KL(q_{\\theta}(Z|S) \\lVert p(Z)) \\leq \\log{p(S)}$$\n",
    "\n",
    "Here, $Z$ is the latent variable, $S$ is the observed SMILES data (evidence), $q_{\\theta}(Z|S)$ (encoder output) is the approximation to the true posterior distribution - $p(Z|S)$, $p_{\\theta}(S|Z)$ is the conditional distribution over $S$ given $Z$ (decoder output), $p(Z)$ is the prior ditribution of the latent variable $Z$, and $p(S)$ is the marginal distribution of the data.\n",
    "\n",
    "The first term is estimated using a single sample drawn from the approximate posterior, whereas the KL term is computed analytically in closed form. The goal of the optimization is to maximize the lower bound, and this can be achieved by minimizing its negative (the loss function):\n",
    "\n",
    "$$L(S;\\theta) = -\\log{p_{\\theta}(S|Z)} + KL(q_{\\theta}(Z|S) \\lVert p(Z))$$\n",
    "\n",
    "Here also the expectation term was replaced by the neg. log-likelihood of $S$ given a single $Z$ drawn from $q_{\\theta}(Z|S)$ as explained above. For a minibatch of samples - $S$, the loss function then becomes:\n",
    "\n",
    "$$L(S;\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m} \\log{p_{\\theta}(S_i|Z_i)} + \\frac{1}{m}\\sum_{i=1}^{m} KL(q_{\\theta}(Z_i|S_i) \\lVert p(Z))=NLL + KL$$\n",
    "\n",
    "Here, $i$ is a particular example from a batch of $m$ samples.\n",
    "\n",
    "Because $q_{\\theta}(Z_i|S_i)$ and $p(Z)$ are both chosen to be diagonal multivariate gaussians and in particular $p(Z) \\sim \\mathcal{N}(0, I)$, the KL divergence term can be computed analytically in closed form:\n",
    "\n",
    "$$KL(q_{\\theta}(Z_i|S_i) \\lVert p(Z)) = \\frac{1}{2}\\left(\\lVert\\mu_q\\lVert^2 - k + tr\\{\\Sigma_q\\} - \\log{|\\Sigma_q|}\\right)=\\frac{1}{2}\\left(\\sum_{j=1}^{k} \\mu_j^2 - k + \\sum_{j=1}^{k} \\nu_j - \\sum_{j=1}^{k} \\log{\\nu_j}\\right)=\\frac{1}{2}\\sum_{j=1}^{k}\\left(\\mu_j^2 - 1 + \\nu_j - \\log{\\nu_j}\\right)$$\n",
    "\n",
    "where $\\mu_j$ and $\\nu_j$ are the $j^{th}$ components of the mean and variance vectors of $q_{\\theta}(Z_i|S_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def compute_loss(model:nn.Module, batch:torch.Tensor) -> tuple:\n",
    "    \"\"\"Computes the NLL and KL loss for a batch of sequences\"\"\"\n",
    "    batch_sz = batch.size(0)\n",
    "    # prepare targets (input shifted 1 step to the left and padded with pad idx)\n",
    "    targets = batch.roll(shifts=-1, dims=1)\n",
    "    targets[:, -1] = model.pad_idx\n",
    "    # prepare predictions\n",
    "    logp, _, mean, logv = model(batch)\n",
    "    preds = logp.permute(0, 2, 1)\n",
    "    # compute NLL (neg. log likelihood) loss for the target sequence (ignore padding idx)\n",
    "    nll_loss = F.nll_loss(preds, targets, ignore_index=model.pad_idx, reduction=\"sum\") / batch_sz\n",
    "    # compute KL loss\n",
    "    kl_loss = -0.5 * torch.sum(1 + logv - mean.pow(2) - logv.exp()) / batch_sz\n",
    "    # compute total loss (averged within a batch)\n",
    "    return (nll_loss, kl_loss)\n",
    "\n",
    "def annealing_func(s:int, m:int, w:float) -> float:\n",
    "    \"\"\"\n",
    "    Controls the final loss function through an adjustable KL weight\n",
    "    -------------------\n",
    "    s - iteration step\n",
    "    m - midpoint (iteration step at which the KL weight is 0.5)\n",
    "    w - width (smoothnes of the sigmoid annealing function around the midpoint)\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-(s - m)/w))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1\n",
    "BATCH_SZ = 128\n",
    "KLW_MIDPOINT = 4000\n",
    "KLW_WIDTH = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Training batch: 0, ELBO: 176.4229, NLL: 176.4156, KL div: 0.4033, KL weight: 0.0180\n",
      "Training batch: 100, ELBO: 105.5564, NLL: 105.0558, KL div: 25.2318, KL weight: 0.0198\n",
      "Training batch: 200, ELBO: 88.2850, NLL: 87.9084, KL div: 17.2122, KL weight: 0.0219\n",
      "Training batch: 300, ELBO: 85.7094, NLL: 85.4159, KL div: 12.1642, KL weight: 0.0241\n",
      "Training batch: 400, ELBO: 85.1872, NLL: 84.8988, KL div: 10.8430, KL weight: 0.0266\n",
      "Training batch: 500, ELBO: 80.7567, NLL: 80.4910, KL div: 9.0627, KL weight: 0.0293\n",
      "Training batch: 600, ELBO: 83.4587, NLL: 83.2181, KL div: 7.4484, KL weight: 0.0323\n",
      "Training batch: 700, ELBO: 81.8639, NLL: 81.6549, KL div: 5.8745, KL weight: 0.0356\n",
      "Training batch: 800, ELBO: 79.1016, NLL: 78.8323, KL div: 6.8741, KL weight: 0.0392\n",
      "Training batch: 900, ELBO: 77.6445, NLL: 77.4106, KL div: 5.4271, KL weight: 0.0431\n",
      "Training batch: 1000, ELBO: 79.7108, NLL: 79.4682, KL div: 5.1141, KL weight: 0.0474\n",
      "Training batch: 1100, ELBO: 73.6171, NLL: 73.3902, KL div: 4.3505, KL weight: 0.0522\n",
      "Training batch: 1200, ELBO: 73.8364, NLL: 73.5821, KL div: 4.4356, KL weight: 0.0573\n",
      "Training batch: 1300, ELBO: 79.9880, NLL: 79.7141, KL div: 4.3492, KL weight: 0.0630\n",
      "Training batch: 1400, ELBO: 76.4295, NLL: 76.1927, KL div: 3.4257, KL weight: 0.0691\n",
      "Training batch: 1500, ELBO: 75.6950, NLL: 75.4536, KL div: 3.1824, KL weight: 0.0759\n",
      "Training batch: 1600, ELBO: 76.3185, NLL: 76.1055, KL div: 2.5614, KL weight: 0.0832\n",
      "Training batch: 1700, ELBO: 73.7255, NLL: 73.4385, KL div: 3.1503, KL weight: 0.0911\n",
      "Training batch: 1800, ELBO: 74.0841, NLL: 73.8260, KL div: 2.5875, KL weight: 0.0998\n",
      "Training batch: 1900, ELBO: 75.4806, NLL: 75.2197, KL div: 2.3909, KL weight: 0.1091\n",
      "Training batch: 2000, ELBO: 74.9804, NLL: 74.7440, KL div: 1.9834, KL weight: 0.1192\n",
      "Training batch: 2100, ELBO: 73.1464, NLL: 72.8750, KL div: 2.0860, KL weight: 0.1301\n",
      "Training batch: 2200, ELBO: 75.1557, NLL: 74.9219, KL div: 1.6483, KL weight: 0.1419\n",
      "Training batch: 2300, ELBO: 73.0536, NLL: 72.7932, KL div: 1.6858, KL weight: 0.1545\n",
      "Training batch: 2400, ELBO: 75.8099, NLL: 75.5409, KL div: 1.6010, KL weight: 0.1680\n",
      "Training batch: 2500, ELBO: 74.5232, NLL: 74.2696, KL div: 1.3904, KL weight: 0.1824\n",
      "Training batch: 2600, ELBO: 72.2325, NLL: 71.9961, KL div: 1.1950, KL weight: 0.1978\n",
      "Training batch: 2700, ELBO: 72.8773, NLL: 72.6515, KL div: 1.0542, KL weight: 0.2142\n",
      "Training batch: 2800, ELBO: 71.3021, NLL: 71.1245, KL div: 0.7671, KL weight: 0.2315\n",
      "Training batch: 2900, ELBO: 71.9301, NLL: 71.7734, KL div: 0.6277, KL weight: 0.2497\n",
      "Training batch: 3000, ELBO: 67.9571, NLL: 67.8383, KL div: 0.4419, KL weight: 0.2689\n",
      "Training batch: 3100, ELBO: 73.0102, NLL: 72.9074, KL div: 0.3555, KL weight: 0.2891\n",
      "Training batch: 3200, ELBO: 68.4357, NLL: 68.3499, KL div: 0.2768, KL weight: 0.3100\n",
      "Training batch: 3300, ELBO: 70.8542, NLL: 70.8157, KL div: 0.1161, KL weight: 0.3318\n",
      "Training batch: 3400, ELBO: 69.1344, NLL: 69.1151, KL div: 0.0542, KL weight: 0.3543\n",
      "Training batch: 3500, ELBO: 71.5873, NLL: 71.5758, KL div: 0.0303, KL weight: 0.3775\n",
      "Training batch: 3600, ELBO: 70.8744, NLL: 70.8694, KL div: 0.0125, KL weight: 0.4013\n",
      "Training batch: 3700, ELBO: 69.5386, NLL: 69.5320, KL div: 0.0155, KL weight: 0.4256\n",
      "Training batch: 3800, ELBO: 70.4293, NLL: 70.4260, KL div: 0.0073, KL weight: 0.4502\n",
      "Training batch: 3900, ELBO: 69.2869, NLL: 69.2844, KL div: 0.0053, KL weight: 0.4750\n",
      "Training batch: 4000, ELBO: 72.3790, NLL: 72.3760, KL div: 0.0060, KL weight: 0.5000\n",
      "Training batch: 4100, ELBO: 67.6893, NLL: 67.6884, KL div: 0.0017, KL weight: 0.5250\n",
      "Training batch: 4200, ELBO: 71.2292, NLL: 71.2280, KL div: 0.0022, KL weight: 0.5498\n",
      "Training batch: 4300, ELBO: 72.2056, NLL: 72.2046, KL div: 0.0016, KL weight: 0.5744\n",
      "Training batch: 4400, ELBO: 69.9901, NLL: 69.9895, KL div: 0.0010, KL weight: 0.5987\n",
      "Training batch: 4500, ELBO: 70.5261, NLL: 70.5256, KL div: 0.0009, KL weight: 0.6225\n",
      "Training batch: 4600, ELBO: 72.7738, NLL: 72.7731, KL div: 0.0011, KL weight: 0.6457\n",
      "Training batch: 4700, ELBO: 73.4164, NLL: 73.4160, KL div: 0.0006, KL weight: 0.6682\n",
      "Training batch: 4800, ELBO: 70.2200, NLL: 70.2196, KL div: 0.0006, KL weight: 0.6900\n",
      "Training batch: 4900, ELBO: 74.3282, NLL: 74.3278, KL div: 0.0006, KL weight: 0.7109\n",
      "Training batch: 5000, ELBO: 72.4062, NLL: 72.4059, KL div: 0.0004, KL weight: 0.7311\n",
      "Training batch: 5100, ELBO: 71.1633, NLL: 71.1629, KL div: 0.0005, KL weight: 0.7503\n",
      "Training batch: 5200, ELBO: 64.1341, NLL: 64.1337, KL div: 0.0005, KL weight: 0.7685\n",
      "Training batch: 5300, ELBO: 72.1972, NLL: 72.1968, KL div: 0.0005, KL weight: 0.7858\n",
      "Training batch: 5400, ELBO: 71.8278, NLL: 71.8275, KL div: 0.0004, KL weight: 0.8022\n",
      "Training batch: 5500, ELBO: 69.6299, NLL: 69.6296, KL div: 0.0004, KL weight: 0.8176\n",
      "Training batch: 5600, ELBO: 70.0844, NLL: 70.0841, KL div: 0.0004, KL weight: 0.8320\n",
      "Training batch: 5700, ELBO: 70.0974, NLL: 70.0971, KL div: 0.0003, KL weight: 0.8455\n",
      "Training batch: 5800, ELBO: 73.3390, NLL: 73.3387, KL div: 0.0003, KL weight: 0.8581\n",
      "Training batch: 5900, ELBO: 73.7280, NLL: 73.7276, KL div: 0.0004, KL weight: 0.8699\n",
      "Training batch: 6000, ELBO: 68.2680, NLL: 68.2677, KL div: 0.0003, KL weight: 0.8808\n",
      "Training batch: 6100, ELBO: 68.8793, NLL: 68.8791, KL div: 0.0003, KL weight: 0.8909\n",
      "Training batch: 6200, ELBO: 70.3089, NLL: 70.3087, KL div: 0.0003, KL weight: 0.9002\n",
      "Training batch: 6300, ELBO: 69.5322, NLL: 69.5320, KL div: 0.0003, KL weight: 0.9089\n",
      "Training batch: 6400, ELBO: 64.9102, NLL: 64.9099, KL div: 0.0004, KL weight: 0.9168\n",
      "Training batch: 6500, ELBO: 65.5303, NLL: 65.5301, KL div: 0.0003, KL weight: 0.9241\n",
      "Training batch: 6600, ELBO: 65.6607, NLL: 65.6604, KL div: 0.0002, KL weight: 0.9309\n",
      "Training batch: 6700, ELBO: 69.0067, NLL: 69.0065, KL div: 0.0002, KL weight: 0.9370\n",
      "Training batch: 6800, ELBO: 66.9051, NLL: 66.9049, KL div: 0.0002, KL weight: 0.9427\n",
      "Training batch: 6900, ELBO: 69.5596, NLL: 69.5594, KL div: 0.0003, KL weight: 0.9478\n",
      "Training batch: 7000, ELBO: 70.0420, NLL: 70.0418, KL div: 0.0002, KL weight: 0.9526\n",
      "Training batch: 7100, ELBO: 68.3452, NLL: 68.3449, KL div: 0.0003, KL weight: 0.9569\n",
      "Training batch: 7200, ELBO: 67.7151, NLL: 67.7149, KL div: 0.0003, KL weight: 0.9608\n",
      "Training batch: 7300, ELBO: 66.1391, NLL: 66.1388, KL div: 0.0002, KL weight: 0.9644\n",
      "Training batch: 7400, ELBO: 68.4992, NLL: 68.4990, KL div: 0.0002, KL weight: 0.9677\n",
      "Training batch: 7500, ELBO: 71.0664, NLL: 71.0662, KL div: 0.0002, KL weight: 0.9707\n",
      "Training batch: 7600, ELBO: 71.4125, NLL: 71.4124, KL div: 0.0002, KL weight: 0.9734\n",
      "Training batch: 7700, ELBO: 69.6132, NLL: 69.6130, KL div: 0.0002, KL weight: 0.9759\n",
      "Training batch: 7800, ELBO: 69.0277, NLL: 69.0275, KL div: 0.0002, KL weight: 0.9781\n",
      "Training batch: 7900, ELBO: 70.3575, NLL: 70.3573, KL div: 0.0002, KL weight: 0.9802\n",
      "Training batch: 8000, ELBO: 66.8388, NLL: 66.8386, KL div: 0.0002, KL weight: 0.9820\n",
      "Training batch: 8100, ELBO: 67.9604, NLL: 67.9602, KL div: 0.0002, KL weight: 0.9837\n",
      "Training batch: 8200, ELBO: 67.1205, NLL: 67.1203, KL div: 0.0002, KL weight: 0.9852\n",
      "Training batch: 8300, ELBO: 66.5098, NLL: 66.5096, KL div: 0.0002, KL weight: 0.9866\n",
      "Training batch: 8400, ELBO: 64.7388, NLL: 64.7386, KL div: 0.0002, KL weight: 0.9879\n",
      "Training batch: 8500, ELBO: 66.0992, NLL: 66.0991, KL div: 0.0002, KL weight: 0.9890\n",
      "Training batch: 8600, ELBO: 66.4352, NLL: 66.4350, KL div: 0.0002, KL weight: 0.9900\n",
      "Training batch: 8700, ELBO: 68.3565, NLL: 68.3563, KL div: 0.0002, KL weight: 0.9910\n",
      "Training batch: 8800, ELBO: 70.8888, NLL: 70.8887, KL div: 0.0002, KL weight: 0.9918\n",
      "Training batch: 8900, ELBO: 67.3743, NLL: 67.3742, KL div: 0.0002, KL weight: 0.9926\n",
      "Training batch: 9000, ELBO: 68.9689, NLL: 68.9687, KL div: 0.0002, KL weight: 0.9933\n",
      "Training batch: 9100, ELBO: 66.7343, NLL: 66.7341, KL div: 0.0002, KL weight: 0.9939\n",
      "Training batch: 9200, ELBO: 67.3787, NLL: 67.3786, KL div: 0.0001, KL weight: 0.9945\n",
      "Training batch: 9300, ELBO: 71.4284, NLL: 71.4280, KL div: 0.0004, KL weight: 0.9950\n",
      "Training batch: 9400, ELBO: 67.8623, NLL: 67.8621, KL div: 0.0002, KL weight: 0.9955\n",
      "Training batch: 9500, ELBO: 69.0987, NLL: 69.0986, KL div: 0.0001, KL weight: 0.9959\n",
      "Training batch: 9600, ELBO: 67.0670, NLL: 67.0668, KL div: 0.0002, KL weight: 0.9963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch: 9700, ELBO: 68.3240, NLL: 68.3238, KL div: 0.0002, KL weight: 0.9967\n",
      "Training batch: 9800, ELBO: 67.4940, NLL: 67.4938, KL div: 0.0002, KL weight: 0.9970\n",
      "Training batch: 9900, ELBO: 61.7412, NLL: 61.7409, KL div: 0.0003, KL weight: 0.9973\n",
      "Training batch: 10000, ELBO: 67.0018, NLL: 67.0016, KL div: 0.0002, KL weight: 0.9975\n",
      "Epoch 1 validation scores: ELBO: 67.5875, NLL: 67.5873, KL div: 0.0002\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=0.001)\n",
    "step = 0\n",
    "for e in range(1, NUM_EPOCHS+1):\n",
    "    print(\"Epoch: %i\" %e)\n",
    "    # train model on the training set\n",
    "    dataloader = tud.DataLoader(train_ds, batch_size=BATCH_SZ, shuffle=True, collate_fn=train_ds.getCollateFn())\n",
    "    # enable dropout\n",
    "    vae.train()\n",
    "#     train_scores = {\"ELBO\":[], \"NLL\":[], \"KL\":[]}\n",
    "    for b, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        # compute loss\n",
    "        nll_loss, kl_loss = compute_loss(vae, batch)\n",
    "        kl_w = annealing_func(step, KLW_MIDPOINT, KLW_WIDTH)\n",
    "        loss = nll_loss + kl_w * kl_loss\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "        # print training progress\n",
    "        if b % 100 == 0:\n",
    "            print(\"Training batch: %i, ELBO: %.4f, NLL: %.4f, KL div: %.4f, KL weight: %.4f\" % (b, \n",
    "                                   loss.item(), nll_loss.item(), kl_loss.item(), kl_w))\n",
    "    # evaluate on the validation set\n",
    "    dataloader = tud.DataLoader(val_ds, batch_size=BATCH_SZ, shuffle=False, collate_fn=val_ds.getCollateFn())\n",
    "    # disable dropout\n",
    "    vae.eval()\n",
    "    val_scores = {\"ELBO\":[], \"NLL\":[], \"KL\":[]}\n",
    "    for b, batch in enumerate(dataloader):\n",
    "        nll_loss, kl_loss = compute_loss(vae, batch)\n",
    "        loss = nll_loss + kl_loss\n",
    "        val_scores[\"ELBO\"].append(loss.item())\n",
    "        val_scores[\"NLL\"].append(nll_loss.item())\n",
    "        val_scores[\"KL\"].append(kl_loss.item())\n",
    "    val_mean = {k:np.mean(v) for k, v in val_scores.items()}\n",
    "    print(\"Epoch %i validation scores: ELBO: %.4f, NLL: %.4f, KL div: %.4f\" % (e, \n",
    "                                   val_mean[\"ELBO\"], val_mean[\"NLL\"], val_mean[\"KL\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), \"saved_models/gru_vae.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MolecularVAE(\n",
       "  (embedding): Embedding(40, 16, padding_idx=0)\n",
       "  (encoder_rnn): GRU(16, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (hidden2mean): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (hidden2logv): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (latent2hidden): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (embedding_droput): Dropout(p=0.0, inplace=False)\n",
       "  (decoder_rnn): GRU(16, 128, num_layers=2, batch_first=True)\n",
       "  (outputs2vocab): Linear(in_features=128, out_features=40, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.load_state_dict(torch.load(\"saved_models/gru_vae.pt\"))\n",
    "# set to inference mode\n",
    "vae.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples = vae.generateSequences(n=32, max_len=150, greedy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CCCS(=O)(=O)N1CCC(N(C)C2=CCC3)Cc2ccc(F)cc2)CC1',\n",
       " 'Nn1nc2c(cc1-c1ccc(Cl)cc1)c1nc(O)o2',\n",
       " 'COC(=O)c1cc(/N=C/C(=O)N(C)C)c(=O)n11C(=O)NC[N+]([O-]',\n",
       " 'Cc1cccc(C(NC(CSc2nnc(CC3CCCCCCC3)co2)cn1',\n",
       " 'CC(N)c1nc2n(C(C)N)N(C)C)ccc2c1NCc1ccc(C)o1',\n",
       " 'COc1cc(C)c(CNC(=O)CC2CC3)ccc2-n1cnc2sc(C)ccc12',\n",
       " 'O=C(NC[C@@H](Nc2ccc(-c3ccc([N@](=O)[O-])cc3)ncnc21',\n",
       " 'NC(=O)[C@@H]1CC[C@H]2c2ccccc2N1C(=O)C1CCN(NC(C)=O)OCC#O)CCCCC1',\n",
       " 'CCOC(=O)c1sccc1C(=O)N1CCN(S(=O)(=O)c2cccc([O+](=O)[O-])c2)CC1',\n",
       " 'O=C1NC(c2ccccc2C(F)(F)F)P(=O)(O)O',\n",
       " 'CN(CCOC#CCCCCCCN1C(=O)C1=C(CC[N+](=O)[O-])C2=O',\n",
       " 'O=C(O)c1cc(Cl)cc(Cc2ccncn2)c1-c1cn(C2CCC2)co1',\n",
       " 'Cc1nc2c(c(-c3ccccc3)nc3ccc(-c4cc(O)cc(C)c3o4)cc(F)c12',\n",
       " 'Cn1nc(Cl)c(Cl)c1NC(=O)CSc1nnc(C)c1C',\n",
       " 'Cc1ccc(NC(=O)CSc2nc(C)c(C[C@@H](C)O)cn2)cc1OC',\n",
       " 'COC(=O)CC(=O)Oc1ccccc1[N+](=O)[O-]',\n",
       " 'CC(C)(Cc1ccnc[nH]c1)C(c1ccc2ccccc21',\n",
       " 'CC1CCCCC1',\n",
       " 'CCCCCCCCC/[N+](=O)[O-])c1ccc2c(c1)OCCO3)C1=Nc2cccc(F)c2)C1',\n",
       " 'Brc1ccc(I)cc1)Nc1cc(Cl)ccc1O',\n",
       " 'CCO(C)[C@](O)(C)CC[C@H](COC(=O)c1c(C2CCCCC2)nc(Cc2ccc(F)c2)N[C@@](Cc2ccco2)C1=O',\n",
       " 'CC(c1cc(S(C)(=O)=O)cc1NS(=O)(=O)c1ccc(Cl)cc1',\n",
       " 'CC(COC1=C(CC(C)C)C2C1CC1)C(=O)C(C)=C1/C2Cc3ccc(F)cc3)C[C@@]2=C([C@@]2(CCC[C@](O)(C)[C@@]2(C)CCC[C@@H]2C[C@]1(C)O[C@@]12C',\n",
       " 'CCC[C@H](CSc1c[nH]c2ccccc2c1=O',\n",
       " 'CCOc1ccc(NC(=O)(OCNC(=O)COc2ccccc2)C(=O)NCc2ccccc21',\n",
       " 'NC1(C)[C@H]2c3ccccc32)[C@@H]1[C@@H](OC)c1ccccc1',\n",
       " 'O=C1CC(c2nnccc(C#N)ccc3C1)c2cn(C)ccc12',\n",
       " 'O=C(COc1ccc(O)cc1)C(=O)O)c1cccc(Cl)c1',\n",
       " 'CN1CCOCC12CC1)c1ccc2nc(Br)cn2C1',\n",
       " 'O=C(COC(=O)Nc1nnn(C)c1)c1ccc(F)cc1',\n",
       " 'CC(=O)NC(C)(C)C(=O)c1c[nH]c2c1cccc(Br)c1)C(C3=O',\n",
       " 'COc1ccc(-c2cccc(NC(=O)C3S4c4csc(C#N)c3)c3)cc2)cc1']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_smiles = [tokenizer.untokenize(vocabulary.decode(s)) for s in samples.tolist()]\n",
    "gen_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": [
       "<?xml version='1.0' encoding='iso-8859-1'?>\n",
       "<svg version='1.1' baseProfile='full'\n",
       "              xmlns='http://www.w3.org/2000/svg'\n",
       "                      xmlns:rdkit='http://www.rdkit.org/xml'\n",
       "                      xmlns:xlink='http://www.w3.org/1999/xlink'\n",
       "                  xml:space='preserve'\n",
       "width='450px' height='150px' viewBox='0 0 450 150'>\n",
       "<!-- END OF HEADER -->\n",
       "<rect style='opacity:1.0;fill:#FFFFFF;stroke:none' width='450' height='150' x='0' y='0'> </rect>\n",
       "<path class='bond-0' d='M 83.6166,83.4176 L 97.9617,90.1747' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-0' d='M 97.9617,90.1747 L 112.307,96.9317' style='fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-1' d='M 124.232,95.5995 L 136.982,86.7454' style='fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-1' d='M 136.982,86.7454 L 149.732,77.8913' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-2' d='M 153.549,77.5723 L 152.22,61.6785' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-2' d='M 152.22,61.6785 L 150.892,45.7848' style='fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-2' d='M 145.915,78.2104 L 144.586,62.3167' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-2' d='M 144.586,62.3167 L 143.258,46.4229' style='fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-3' d='M 149.732,77.8913 L 184.385,94.2139' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-4' d='M 184.385,94.2139 L 215.847,72.365' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-5' d='M 219.664,72.0459 L 218.335,56.1522' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-5' d='M 218.335,56.1522 L 217.007,40.2585' style='fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-5' d='M 212.03,72.6841 L 210.701,56.7903' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-5' d='M 210.701,56.7903 L 209.373,40.8966' style='fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-6' d='M 215.847,72.365 L 230.192,79.122' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-6' d='M 230.192,79.122 L 244.537,85.8791' style='fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-7' d='M 256.462,84.5469 L 269.212,75.6928' style='fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-7' d='M 269.212,75.6928 L 281.962,66.8387' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-8' d='M 281.962,66.8387 L 278.771,28.6671' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-8' d='M 289.118,60.4748 L 286.884,33.7547' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-16' d='M 281.962,66.8387 L 316.615,83.1613' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-9' d='M 278.771,28.6671 L 310.233,6.81818' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-10' d='M 310.233,6.81818 L 344.886,23.1408' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-10' d='M 312.167,16.1971 L 336.424,27.623' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-11' d='M 344.886,23.1408 L 348.077,61.3124' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-12' d='M 348.077,61.3124 L 316.615,83.1613' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-12' d='M 338.988,58.2973 L 316.964,73.5915' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-13' d='M 316.615,83.1613 L 317.917,98.7358' style='fill:none;fill-rule:evenodd;stroke:#000000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-13' d='M 317.917,98.7358 L 319.218,114.31' style='fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-14' d='M 325.944,128.459 L 336.404,133.385' style='fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-14' d='M 336.404,133.385 L 346.863,138.312' style='fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-14' d='M 329.209,121.528 L 339.668,126.455' style='fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-14' d='M 339.668,126.455 L 350.128,131.382' style='fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-15' d='M 312.034,126.73 L 303.808,132.442' style='fill:none;fill-rule:evenodd;stroke:#0000FF;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<path class='bond-15' d='M 303.808,132.442 L 295.582,138.155' style='fill:none;fill-rule:evenodd;stroke:#FF0000;stroke-width:2px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1' />\n",
       "<text x='112.307' y='106.124' style='font-size:12px;font-style:normal;font-weight:normal;fill-opacity:1;stroke:none;font-family:sans-serif;text-anchor:start;fill:#FF0000' ><tspan>O</tspan></text>\n",
       "<text x='140.578' y='46.1039' style='font-size:12px;font-style:normal;font-weight:normal;fill-opacity:1;stroke:none;font-family:sans-serif;text-anchor:start;fill:#FF0000' ><tspan>O</tspan></text>\n",
       "<text x='206.694' y='40.5776' style='font-size:12px;font-style:normal;font-weight:normal;fill-opacity:1;stroke:none;font-family:sans-serif;text-anchor:start;fill:#FF0000' ><tspan>O</tspan></text>\n",
       "<text x='244.537' y='95.0717' style='font-size:12px;font-style:normal;font-weight:normal;fill-opacity:1;stroke:none;font-family:sans-serif;text-anchor:start;fill:#FF0000' ><tspan>O</tspan></text>\n",
       "<text x='312.034' y='128.355' style='font-size:12px;font-style:normal;font-weight:normal;fill-opacity:1;stroke:none;font-family:sans-serif;text-anchor:start;fill:#0000FF' ><tspan>N</tspan><tspan style='baseline-shift:super;font-size:9px;'>+</tspan><tspan></tspan></text>\n",
       "<text x='348.496' y='144.04' style='font-size:12px;font-style:normal;font-weight:normal;fill-opacity:1;stroke:none;font-family:sans-serif;text-anchor:start;fill:#FF0000' ><tspan>O</tspan></text>\n",
       "<text x='281.104' y='150.204' style='font-size:12px;font-style:normal;font-weight:normal;fill-opacity:1;stroke:none;font-family:sans-serif;text-anchor:start;fill:#FF0000' ><tspan>O</tspan><tspan style='baseline-shift:super;font-size:9px;'>-</tspan><tspan></tspan></text>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<rdkit.Chem.rdchem.Mol at 0x7f9e7001d490>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Chem.MolFromSmiles(gen_smiles[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit ERROR: [18:20:59] SMILES Parse Error: extra close parentheses while parsing: CCCS(=O)(=O)N1CCC(N(C)C2=CCC3)Cc2ccc(F)cc2)CC1\n",
      "RDKit ERROR: [18:20:59] \n",
      "RDKit ERROR: \n",
      "RDKit ERROR: ****\n",
      "RDKit ERROR: Pre-condition Violation\n",
      "RDKit ERROR: no atoms\n",
      "RDKit ERROR: Violation occurred on line 173 in file /opt/conda/conda-bld/rdkit_1571980026485/work/Code/GraphMol/ROMol.cpp\n",
      "RDKit ERROR: Failed Expression: getNumAtoms() > 0\n",
      "RDKit ERROR: ****\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [18:20:59] SMILES Parse Error: unclosed ring for input: 'Nn1nc2c(cc1-c1ccc(Cl)cc1)c1nc(O)o2'\n",
      "RDKit ERROR: [18:20:59] SMILES Parse Error: extra open parentheses for input: 'COC(=O)c1cc(/N=C/C(=O)N(C)C)c(=O)n11C(=O)NC[N+]([O-]'\n",
      "RDKit ERROR: [18:20:59] SMILES Parse Error: extra open parentheses for input: 'Cc1cccc(C(NC(CSc2nnc(CC3CCCCCCC3)co2)cn1'\n",
      "RDKit ERROR: [18:20:59] SMILES Parse Error: extra close parentheses while parsing: CC(N)c1nc2n(C(C)N)N(C)C)ccc2c1NCc1ccc(C)o1\n",
      "RDKit ERROR: [18:20:59] \n",
      "RDKit ERROR: \n",
      "RDKit ERROR: ****\n",
      "RDKit ERROR: Pre-condition Violation\n",
      "RDKit ERROR: no atoms\n",
      "RDKit ERROR: Violation occurred on line 173 in file /opt/conda/conda-bld/rdkit_1571980026485/work/Code/GraphMol/ROMol.cpp\n",
      "RDKit ERROR: Failed Expression: getNumAtoms() > 0\n",
      "RDKit ERROR: ****\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [18:20:59] SMILES Parse Error: unclosed ring for input: 'COc1cc(C)c(CNC(=O)CC2CC3)ccc2-n1cnc2sc(C)ccc12'\n",
      "RDKit ERROR: [18:20:59] SMILES Parse Error: extra open parentheses for input: 'O=C(NC[C@@H](Nc2ccc(-c3ccc([N@](=O)[O-])cc3)ncnc21'\n",
      "RDKit ERROR: [18:20:59] SMILES Parse Error: extra close parentheses while parsing: NC(=O)[C@@H]1CC[C@H]2c2ccccc2N1C(=O)C1CCN(NC(C)=O)OCC#O)CCCCC1\n",
      "RDKit ERROR: [18:20:59] \n",
      "RDKit ERROR: \n",
      "RDKit ERROR: ****\n",
      "RDKit ERROR: Pre-condition Violation\n",
      "RDKit ERROR: no atoms\n",
      "RDKit ERROR: Violation occurred on line 173 in file /opt/conda/conda-bld/rdkit_1571980026485/work/Code/GraphMol/ROMol.cpp\n",
      "RDKit ERROR: Failed Expression: getNumAtoms() > 0\n",
      "RDKit ERROR: ****\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [18:20:59] Explicit valence for atom # 24 O, 3, is greater than permitted\n",
      "RDKit ERROR: [18:20:59] SMILES Parse Error: unclosed ring for input: 'O=C1NC(c2ccccc2C(F)(F)F)P(=O)(O)O'\n",
      "RDKit ERROR: [18:20:59] SMILES Parse Error: extra open parentheses for input: 'CN(CCOC#CCCCCCCN1C(=O)C1=C(CC[N+](=O)[O-])C2=O'\n",
      "RDKit ERROR: [18:20:59] Can't kekulize mol.  Unkekulized atoms: 17 18 24\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [18:20:59] SMILES Parse Error: extra open parentheses for input: 'Cc1nc2c(c(-c3ccccc3)nc3ccc(-c4cc(O)cc(C)c3o4)cc(F)c12'\n",
      "RDKit ERROR: [18:20:59] Can't kekulize mol.  Unkekulized atoms: 13 14 15 16 18\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [18:20:59] SMILES Parse Error: extra open parentheses for input: 'CC(C)(Cc1ccnc[nH]c1)C(c1ccc2ccccc21'\n",
      "RDKit ERROR: [18:20:59] SMILES Parse Error: extra close parentheses while parsing: CCCCCCCCC/[N+](=O)[O-])c1ccc2c(c1)OCCO3)C1=Nc2cccc(F)c2)C1\n",
      "RDKit ERROR: [18:20:59] \n",
      "RDKit ERROR: \n",
      "RDKit ERROR: ****\n",
      "RDKit ERROR: Pre-condition Violation\n",
      "RDKit ERROR: no atoms\n",
      "RDKit ERROR: Violation occurred on line 173 in file /opt/conda/conda-bld/rdkit_1571980026485/work/Code/GraphMol/ROMol.cpp\n",
      "RDKit ERROR: Failed Expression: getNumAtoms() > 0\n",
      "RDKit ERROR: ****\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [18:20:59] SMILES Parse Error: extra close parentheses while parsing: Brc1ccc(I)cc1)Nc1cc(Cl)ccc1O\n",
      "RDKit ERROR: [18:20:59] \n",
      "RDKit ERROR: \n",
      "RDKit ERROR: ****\n",
      "RDKit ERROR: Pre-condition Violation\n",
      "RDKit ERROR: no atoms\n",
      "RDKit ERROR: Violation occurred on line 173 in file /opt/conda/conda-bld/rdkit_1571980026485/work/Code/GraphMol/ROMol.cpp\n",
      "RDKit ERROR: Failed Expression: getNumAtoms() > 0\n",
      "RDKit ERROR: ****\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [18:20:59] SMILES Parse Error: extra open parentheses for input: 'CCO(C)[C@](O)(C)CC[C@H](COC(=O)c1c(C2CCCCC2)nc(Cc2ccc(F)c2)N[C@@](Cc2ccco2)C1=O'\n",
      "RDKit ERROR: [18:20:59] SMILES Parse Error: extra open parentheses for input: 'CC(c1cc(S(C)(=O)=O)cc1NS(=O)(=O)c1ccc(Cl)cc1'\n",
      "RDKit ERROR: [18:20:59] SMILES Parse Error: extra close parentheses while parsing: CC(COC1=C(CC(C)C)C2C1CC1)C(=O)C(C)=C1/C2Cc3ccc(F)cc3)C[C@@]2=C([C@@]2(CCC[C@](O)(C)[C@@]2(C)CCC[C@@H]2C[C@]1(C)O[C@@]12C\n",
      "RDKit ERROR: [18:20:59] \n",
      "RDKit ERROR: \n",
      "RDKit ERROR: ****\n",
      "RDKit ERROR: Pre-condition Violation\n",
      "RDKit ERROR: no atoms\n",
      "RDKit ERROR: Violation occurred on line 173 in file /opt/conda/conda-bld/rdkit_1571980026485/work/Code/GraphMol/ROMol.cpp\n",
      "RDKit ERROR: Failed Expression: getNumAtoms() > 0\n",
      "RDKit ERROR: ****\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [18:20:59] SMILES Parse Error: extra open parentheses for input: 'CCC[C@H](CSc1c[nH]c2ccccc2c1=O'\n",
      "RDKit ERROR: [18:20:59] SMILES Parse Error: extra open parentheses for input: 'CCOc1ccc(NC(=O)(OCNC(=O)COc2ccccc2)C(=O)NCc2ccccc21'\n",
      "RDKit ERROR: [18:20:59] SMILES Parse Error: extra close parentheses while parsing: NC1(C)[C@H]2c3ccccc32)[C@@H]1[C@@H](OC)c1ccccc1\n",
      "RDKit ERROR: [18:20:59] \n",
      "RDKit ERROR: \n",
      "RDKit ERROR: ****\n",
      "RDKit ERROR: Pre-condition Violation\n",
      "RDKit ERROR: no atoms\n",
      "RDKit ERROR: Violation occurred on line 173 in file /opt/conda/conda-bld/rdkit_1571980026485/work/Code/GraphMol/ROMol.cpp\n",
      "RDKit ERROR: Failed Expression: getNumAtoms() > 0\n",
      "RDKit ERROR: ****\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [18:20:59] SMILES Parse Error: unclosed ring for input: 'O=C1CC(c2nnccc(C#N)ccc3C1)c2cn(C)ccc12'\n",
      "RDKit ERROR: [18:20:59] SMILES Parse Error: extra close parentheses while parsing: O=C(COc1ccc(O)cc1)C(=O)O)c1cccc(Cl)c1\n",
      "RDKit ERROR: [18:20:59] \n",
      "RDKit ERROR: \n",
      "RDKit ERROR: ****\n",
      "RDKit ERROR: Pre-condition Violation\n",
      "RDKit ERROR: no atoms\n",
      "RDKit ERROR: Violation occurred on line 173 in file /opt/conda/conda-bld/rdkit_1571980026485/work/Code/GraphMol/ROMol.cpp\n",
      "RDKit ERROR: Failed Expression: getNumAtoms() > 0\n",
      "RDKit ERROR: ****\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [18:20:59] SMILES Parse Error: extra close parentheses while parsing: CN1CCOCC12CC1)c1ccc2nc(Br)cn2C1\n",
      "RDKit ERROR: [18:20:59] \n",
      "RDKit ERROR: \n",
      "RDKit ERROR: ****\n",
      "RDKit ERROR: Pre-condition Violation\n",
      "RDKit ERROR: no atoms\n",
      "RDKit ERROR: Violation occurred on line 173 in file /opt/conda/conda-bld/rdkit_1571980026485/work/Code/GraphMol/ROMol.cpp\n",
      "RDKit ERROR: Failed Expression: getNumAtoms() > 0\n",
      "RDKit ERROR: ****\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [18:20:59] SMILES Parse Error: extra close parentheses while parsing: CC(=O)NC(C)(C)C(=O)c1c[nH]c2c1cccc(Br)c1)C(C3=O\n",
      "RDKit ERROR: [18:20:59] \n",
      "RDKit ERROR: \n",
      "RDKit ERROR: ****\n",
      "RDKit ERROR: Pre-condition Violation\n",
      "RDKit ERROR: no atoms\n",
      "RDKit ERROR: Violation occurred on line 173 in file /opt/conda/conda-bld/rdkit_1571980026485/work/Code/GraphMol/ROMol.cpp\n",
      "RDKit ERROR: Failed Expression: getNumAtoms() > 0\n",
      "RDKit ERROR: ****\n",
      "RDKit ERROR: \n",
      "RDKit ERROR: [18:20:59] SMILES Parse Error: extra close parentheses while parsing: COc1ccc(-c2cccc(NC(=O)C3S4c4csc(C#N)c3)c3)cc2)cc1\n"
     ]
    }
   ],
   "source": [
    "gen_mols = [Chem.MolFromSmiles(s) for s in gen_smiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " <rdkit.Chem.rdchem.Mol at 0x7f9e700a2a30>,\n",
       " <rdkit.Chem.rdchem.Mol at 0x7f9e700a2e40>,\n",
       " None,\n",
       " <rdkit.Chem.rdchem.Mol at 0x7f9e700a2e90>,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " <rdkit.Chem.rdchem.Mol at 0x7f9e7001d0d0>,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_mols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O=C(/C=C/c1ccc2c(c1)C(=O)N(Cc1ccccc1)C1(CCNCC1)O2)NO'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try sampling from a posterior of a sample from the test set\n",
    "tokenizer.untokenize(vocabulary.decode(test_ds[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logp, z, mean, logv = vae(test_ds[0].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.repeat(4, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1, 23, 23,  ...,  0,  0,  0],\n",
       "        [ 1, 23, 23,  ...,  0,  0,  0],\n",
       "        [ 1, 23, 23,  ...,  0,  0,  0],\n",
       "        ...,\n",
       "        [ 1, 23, 29,  ...,  0,  0,  0],\n",
       "        [ 1, 25, 35,  ...,  0,  0,  0],\n",
       "        [ 1, 23, 23,  ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples2 = vae.generateSequences(z=z.repeat(16, 1), max_len=150, greedy=False)\n",
    "samples2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CC(C)NCn1c(C[C@@H](N)Cc2ccco2)no1',\n",
       " 'CCC1(C)[C@H]1CCCN(CCc1cccc(OP(C)(=O)=O)c1)C(S)C',\n",
       " 'CCCN1CCC2(CCS(S(=O)(=O)c3ccccc3-2)C(=O)c2ccccc2)C(=O)N1',\n",
       " 'O=C(Cc1ccc(Br)c(Cl)c1)N(C(=O)c1ccccc1)C12CC3CN(C(=O)N[C@H]3C=CCC4)C1',\n",
       " 'CC(C)c1cc(c2cc(=O)c3c(c1CC(=O)O)C12CCCCC32',\n",
       " 'CC(C)Cc1ccc(-c2cccc(-n3nc(C)c(OC)c3)cc2)s1',\n",
       " 'CN(C[C@@H]1CC[C@H](CC)CCCNC(=O)c2ccc(N(C)C)cc1)C(=O)OC2N(CCNC1CC[C@@H]2CO[C@H](CO)[C@@H]1CCCC1)C(=O)N2',\n",
       " 'CC1=CNCCN(CCOCC=O)(C(=O)Nc2ccccc2)(F)N1Cc1ccccc1',\n",
       " 'CC(O)=CC[C@@H](Cc1ccccc1)C1',\n",
       " 'O=P1(O)C2(C)CCC1C(N)=O',\n",
       " 'COP(=O)(Oc1ccccc1-c1cccnc1)c1cc(Br)cc(CO)n1',\n",
       " 'CCc1nn(-c2ccccc2)s1',\n",
       " 'COCCNS(=O)(=O)c1ccc(/C=C/C(=O)COC2O)o1',\n",
       " 'CO[C@@H]1CCC[C@]2(O)Cc2cc(Cc3c(cccc3[N+](=O)[O-])CC2)ccc1O',\n",
       " 'Fc1ccc(CNC(=O)CCCN2CCOCC2)cc(OCCNC)c1',\n",
       " 'CCOC(=O)Nc1c(C(=N)N)c(CCNC(=O)COC2=O)c2c(c(C)c3c1OCC3)CC(=O)c1ccccc1']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_mols2 = [tokenizer.untokenize(vocabulary.decode(s)) for s in samples2.tolist()]\n",
    "gen_mols2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.875px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
